
@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv:2302.13971},
  year={2023}
}

@article{blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv:2301.12597},
  year={2023}
}

@article{gpt4,
  title={GPT-4 technical report},
  author={OpenAI},
  journal={arXiv:2303.08774},
  year={2023}
}
@inproceedings{patraucean2024perception,
  title={Perception test: A diagnostic benchmark for multimodal video models},
  author={Patraucean, Viorica and Smaira, Lucas and Gupta, Ankush and Recasens, Adria and Markeeva, Larisa and Banarse, Dylan and Koppula, Skanda and Malinowski, Mateusz and Yang, Yi and Doersch, Carl and others},
  booktitle={NeurIPS},
  year={2024}
}

@article{fu2024video,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv:2405.21075},
  year={2024}
}

@inproceedings{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
  year={2024}
}
@article{zhang2024vcr,
  title={VCR: Visual Caption Restoration},
  author={Zhang, Tianyu and Wang, Suyuchen and Li, Lu and Zhang, Ge and Taslakian, Perouz and Rajeswar, Sai and Fu, Jie and Liu, Bang and Bengio, Yoshua},
  journal={arXiv:2406.06462},
  year={2024}
}

@article{mmtbench,
  title={MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI}, 
  author={Kaining Ying and Fanqing Meng and Jin Wang and Zhiqian Li and Han Lin and Yue Yang and Hao Zhang and Wenbo Zhang and Yuqi Lin and Shuo Liu and Jiayi Lei and Quanfeng Lu and Runjian Chen and Peng Xu and Renrui Zhang and Haozhe Zhang and Peng Gao and Yali Wang and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao},
  year={2024},
  journal={arXiv:2404.16006},
}

@article{MMBench,
    author  = {Yuan Liu and Haodong Duan and Yuanhan Zhang, Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and Kai Chen and Dahua Lin},
    journal = {arXiv:2307.06281},
    title   = {MMBench: Is Your Multi-modal Model an All-around Player?},
    year    = {2023},
}

@article{guan2023hallusionbench,
  title={HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination \& Visual Illusion in Large Vision-Language Models}, 
  author={Tianrui Guan and Fuxiao Liu and Xiyang Wu and Ruiqi Xian and Zongxia Li and Xiaoyu Liu and Xijun Wang and Lichang Chen and Furong Huang and Yaser Yacoob and Dinesh Manocha and Tianyi Zhou},
  year={2023},
  journal={arXiv:2310.14566},
}

@misc{grok15,
  author = {{X.AI}},
  title = {Grok-1.5 vision preview},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}},
}

@misc{grok2,
  author = {{X.AI}},
  title = {Grok-2 Beta Release},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-2}},
}

@article{chen2024we,
  title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv:2403.20330},
  year={2024}
}

@inproceedings{yu2024mm,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  booktitle={ICML},
  year={2024},
}

@inproceedings{flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{lin2024vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26689--26699},
  year={2024}
}

@article{li2024omnicorpus,
  title={OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text},
  author={Li, Qingyun and Chen, Zhe and Wang, Weiyun and Wang, Wenhai and Ye, Shenglong and Jin, Zhenjiang and Chen, Guanzhou and He, Yinan and Gao, Zhangwei and Cui, Erfei and others},
  journal={arXiv preprint arXiv:2406.08418},
  year={2024}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    year = {2023}
}
@article{fang2023data,
  title={Data filtering networks},
  author={Fang, Alex and Jose, Albin Madappally and Jain, Amit and Schmidt, Ludwig and Toshev, Alexander and Shankar, Vaishaal},
  journal={arXiv:2309.17425},
  year={2023}
}

@article{xu2023lvlm,
  title={LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv:2306.09265},
  year={2023}
}
@inproceedings{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}
@article{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv:2304.08485},
  year={2023}
}

@article{minigpt-4,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv:2304.10592},
  year={2023}
}
@article{yang2023dawn,
  title={The dawn of lmms: Preliminary explorations with gpt-4v (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv:2309.17421},
  year={2023}
}
@article{plasticpollution,
    author = {Hannah Ritchie and Veronika Samborska and Max Roser},
    title = {Plastic pollution},
    journal = {Our World in Data},
    year = {2023},
    note = {https://ourworldindata.org/plastic-pollution}
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@article{bai2023touchstone,
  title={Touchstone: Evaluating vision-language models by language models},
  author={Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.16890},
  year={2023}
}
@article{li2023seed,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}


@article{liu2023mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}



@article{yue2023mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  journal={arXiv:2311.16502},
  year={2023}
}
@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  journal={arXiv:2303.16199},
  year={2023}
}

@inproceedings{DBLP:conf/nips/RafailovSMMEF23,
  author       = {Rafael Rafailov and
                  Archit Sharma and
                  Eric Mitchell and
                  Christopher D. Manning and
                  Stefano Ermon and
                  Chelsea Finn},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Direct Preference Optimization: Your Language Model is Secretly a
                  Reward Model},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/RafailovSMMEF23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
  journal={arXiv:2305.06500},
  year={2023}
}
@article{mplug-owl,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv:2304.14178},
  year={2023}
}

@article{gao2023llama,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv:2304.15010},
  year={2023}
}



@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv:2305.03726},
  year={2023}
}

@article{su2023pandagpt,
  title={Pandagpt: One model to instruction-follow them all},
  author={Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
  journal={arXiv:2305.16355},
  year={2023}
}

@article{wang2023makes,
  title={What Makes for Good Visual Tokenizers for Large Language Models?},
  author={Wang, Guangzhi and Ge, Yixiao and Ding, Xiaohan and Kankanhalli, Mohan and Shan, Ying},
  journal={arXiv:2305.12223},
  year={2023}
}

@article{li2023evaluating,
  title={Evaluating object hallucination in large vision-language models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv:2305.10355},
  year={2023}
}

@article{cococaption,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv:1504.00325},
  year={2015}
}

@article{krishna2017visualvg,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@article{laion5b,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={arXiv:2210.08402},
  year={2022}
}

@article{laioncoco,
  title={Laion coco: 600m synthetic captions from laion2b-en.},
  author={Schuhmann, Christoph and Köpf, Andreas and Vencu, Richard and Coombes, Theo and Beaumont, Romain},
  journal = {https://laion.ai/blog/laion-coco/},
  year={2022}
}

@article{datacomp,
  title={DataComp: In search of the next generation of multimodal datasets},
  author={Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others},
  journal={arXiv:2304.14108},
  year={2023}
}

@misc{coyo,
  title         = {COYO-700M: Image-Text Pair Dataset},
  author        = {Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
  year          = {2022},
  url  = {https://github.com/kakaobrain/coyo-dataset},
}

@inproceedings{cc12m,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{cc3m,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={ACL},
  year={2018}
}

@inproceedings{sbu,
  title={Im2text: Describing images using 1 million captioned photographs},
  author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
  booktitle={NeurIPS},
  year={2011}
}

@article{chen2022pali,
  title={Pali: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  journal={arXiv:2209.06794},
  year={2022}
}

@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={ICML},
  year={2022},
}

@article{ouyang2022intructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={NeurIPS},
  year={2022}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv:2305.10403},
  year={2023}
}

@inproceedings{agrawal2019nocaps,
  title={nocaps: novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle={ICCV},
  year={2019}
}
@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{sidorov2020textcaps,
  title={Textcaps: a dataset for image captioning with reading comprehension},
  author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  booktitle={ECCV},
  year={2020},
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  year={2009},
}

@article{li2023seedbench,
  title={SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension}, 
  author={Bohao Li and Rui Wang and Guangzhi Wang and Yuying Ge and Yixiao Ge and Ying Shan},
  journal={arXiv:2307.16125},
  year={2023}
}
@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv:2306.13394},
  year={2023}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv:2205.01068},
  year={2022}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv:2306.05685},
  year={2023}
}
@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv:1810.04805},
  year={2018}
}

@article{raffel2020t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020},
}
@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2022}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv:2106.08254},
  year={2021}
}

@article{bai2022ofasys,
  title={OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models},
  author={Bai, Jinze and Men, Rui and Yang, Hao and Ren, Xuancheng and Dang, Kai and Zhang, Yichang and Zhou, Xiaohuan and Wang, Peng and Tan, Sinan and Yang, An and others},
  journal={arXiv:2212.04408},
  year={2022}
}
@inproceedings{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle={ICML},
  year={2020},
}
@article{huang2023language,
  title={Language is not all you need: Aligning perception with language models},
  author={Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  journal={arXiv:2302.14045},
  year={2023}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014},
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  year={2004}
}
@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={CVPR},
  year={2015}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv:2106.09685},
  year={2021}
}

@software{openclip,
  author       = {Ilharco, Gabriel and Wortsman, Mitchell and Wightman, Ross and Gordon, Cade and Carlini, Nicholas and Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Namkoong, Hongseok and Miller, John and Hajishirzi, Hannaneh and Farhadi, Ali and Schmidt, Ludwig},
  title = {OpenCLIP},
  year = {2021},
  url = {https://doi.org/10.5281/zenodo.5143773}
}

@inproceedings{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle={ICLR},
  year={2021}
}

@misc{xu2023expertprompting,
      title={ExpertPrompting: Instructing Large Language Models to be Distinguished Experts}, 
      author={Benfeng Xu and An Yang and Junyang Lin and Quan Wang and Chang Zhou and Yongdong Zhang and Zhendong Mao},
      year={2023},
      eprint={2305.14688},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qwen7b,
    title = {Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts)},
    author={Qwen},
    url = {https://github.com/QwenLM/Qwen-7B},
    year = {2023}
}

@misc{chatml,
    title = {ChatML documents},
    author={Openai},
    url={https://github.com/openai/openai-python/blob/main/chatml.md},
    year={2024},
}
@article{touchstone,
      title={TouchStone: Evaluating Vision-Language Models by Language Models}, 
      author={Shuai Bai and Shusheng Yang and Jinze Bai and Peng Wang and Xingxuan Zhang and Junyang Lin and Xinggang Wang and Chang Zhou and Jingren Zhou},
      journal={arXiv:2308.16890},
      year={2023},
}
@article{hong2023cogagent,
  title={Cogagent: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv:2312.08914},
  year={2023}
}

@misc{fuyu-8b,
  author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
  title = {Introducing our Multimodal Models},
  url = {https://www.adept.ai/blog/fuyu-8b},
  year = {2023}
}

@inproceedings{lee2023pix2struct,
  title={Pix2struct: Screenshot parsing as pretraining for visual language understanding},
  author={Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  booktitle={International Conference on Machine Learning},
  pages={18893--18912},
  year={2023},
  organization={PMLR}
}
@article{kosmos2,
  title={Kosmos-2: Grounding Multimodal Large Language Models to the World},
  author={Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
  journal={arXiv:2306.14824},
  year={2023}
}

@article{mPLUG-DocOwl,
  title={mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding},
  author={Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Dan, Yuhao and Zhao, Chenlin and Xu, Guohai and Li, Chenliang and Tian, Junfeng and others},
  journal={arXiv:2307.02499},
  year={2023}
}

@article{shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal={arXiv:2306.15195},
  year={2023}
}

@article{bubogpt,
  title={Bubogpt: Enabling visual grounding in multi-modal llms},
  author={Zhao, Yang and Lin, Zhijie and Zhou, Daquan and Huang, Zilong and Feng, Jiashi and Kang, Bingyi},
  journal={arXiv:2307.08581},
  year={2023}
}

@inproceedings{gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{vg,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  booktitle={IJCV},
  year={2017},
}

@inproceedings{VQAv2,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{dvqa,
  title={Dvqa: Understanding data visualizations via question answering},
  author={Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{ocrvqa,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={ICDAR},
  year={2019},
}

@inproceedings{docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={WACV},
  year={2021}
}

@article{kosmos,
  title={Language is not all you need: Aligning perception with language models},
  author={Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  journal={arXiv:2302.14045},
  year={2023}
}

@article{chen2023pali,
  title={PaLI-X: On Scaling up a Multilingual Vision and Language Model},
  author={Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and others},
  journal={arXiv preprint arXiv:2305.18565},
  year={2023}
}
@article{videollama,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv:2306.02858},
  year={2023}
}

@inproceedings{refcoco,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={EMNLP},
  year={2014}
}

@inproceedings{refcocog,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={CVPR},
  year={2016}
}

@article{emu,
  title={Generative pretraining in multimodality},
  author={Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  journal={arXiv:2307.05222},
  year={2023}
}

@inproceedings{synthdog,
  title={Ocr-free document understanding transformer},
  author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle={ECCV},
  year={2022},
}

@article{coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv:2205.01917},
  year={2022}
}

@article{unified_io,
  title={Unified-io: A unified model for vision, language, and multi-modal tasks},
  author={Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  journal={arXiv:2206.08916},
  year={2022}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021},
}

@article{beit3,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv:2208.10442},
  year={2022}
}

@article{one-peace,
  title={ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities},
  author={Wang, Peng and Wang, Shijie and Lin, Junyang and Bai, Shuai and Zhou, Xiaohuan and Zhou, Jingren and Wang, Xinggang and Zhou, Chang},
  journal={arXiv:2305.11172},
  year={2023}
}

@inproceedings{imagebind,
  title={Imagebind: One embedding space to bind them all},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{unit,
  title={Unit: Multimodal multitask learning with a unified transformer},
  author={Hu, Ronghang and Singh, Amanpreet},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{flava,
  title={Flava: A foundational language and vision alignment model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{uni-perceiver,
  title={Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks},
  author={Zhu, Xizhou and Zhu, Jinguo and Li, Hao and Wu, Xiaoshi and Li, Hongsheng and Wang, Xiaohua and Dai, Jifeng},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{vilbert,
  title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author={Jiasen Lu and Dhruv Batra and Devi Parikh and Stefan Lee},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{vlbert,
  title={VL-BERT: Pre-training of Generic Visual-Linguistic Representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{uniter,
  title={UNITER: UNiversal Image-TExt Representation Learning},
  author={Yen-Chun Chen and Linjie Li and Licheng Yu and Ahmed El Kholy and Faisal Ahmed and Zhe Gan and Yu Cheng and Jingjing Liu},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{oscar,
  title={Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks},
  author={Xiujun Li and Xi Yin and Chunyuan Li and Xiaowei Hu and Pengchuan Zhang and Lei Zhang and Lijuan Wang and Houdong Hu and Li Dong and Furu Wei and Yejin Choi and Jianfeng Gao},
  booktitle={ECCV},
  year={2020}
}

@article{pixelbert,
  title={Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers},
  author={Zhicheng Huang and Zhaoyang Zeng and Bei Liu and Dongmei Fu and Jianlong Fu},
  journal={ArXiv},
  year={2020},
}

@inproceedings{villa,
  title={Large-scale adversarial training for vision-and-language representation learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{vlt5,
  title={Unifying Vision-and-Language Tasks via Text Generation},
  author={Jaemin Cho and Jie Lei and Haochen Tan and Mohit Bansal},
  booktitle={ICML},
  year={2021}
}

@article{visualbert,
  title={VisualBERT: A Simple and Performant Baseline for Vision and Language},
  author={Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
  journal={arXiv:1908.03557},
  year={2019},
}

@article{interbert,
  title={Interbert: Vision-and-language interaction for multi-modal pretraining},
  author={Lin, Junyang and Yang, An and Zhang, Yichang and Liu, Jie and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv:2003.13198},
  year={2020}
}

@inproceedings{vinvl,
  title={VinVL: Revisiting Visual Representations in Vision-Language Models},
  author={Pengchuan Zhang and Xiujun Li and Xiaowei Hu and Jianwei Yang and Lei Zhang and Lijuan Wang and Yejin Choi and Jianfeng Gao},
  booktitle={CVPR},
  year={2021},
}

@inproceedings{unimo,
  author    = {Wei Li and
               Can Gao and
               Guocheng Niu and
               Xinyan Xiao and
               Hao Liu and
               Jiachen Liu and
               Hua Wu and
               Haifeng Wang},
  title     = {{UNIMO:} Towards Unified-Modal Understanding and Generation via Cross-Modal
               Contrastive Learning},
  booktitle = {ACL},
  year      = {2021}
}

@inproceedings{m6,
  title={M6: A chinese multimodal pretrainer},
  author={Lin, Junyang and Men, Rui and Yang, An and Zhou, Chang and Ding, Ming and Zhang, Yichang and Wang, Peng and Wang, Ang and Jiang, Le and Jia, Xianyan and others},
  booktitle={KDD},
  year={2021}
}

@inproceedings{vilt,
  title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author={Wonjae Kim and Bokyung Son and Ildoo Kim},
  booktitle={ICML},
  year={2021}
}

@inproceedings{albef,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath R and Gotmare, Akhilesh Deepak and Joty, Shafiq and Xiong, Caiming and Hoi, Steven},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{blip,
  title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Junnan Li and Dongxu Li and Caiming Xiong and Steven C. H. Hoi},
  booktitle={ICML},
  year={2022}
}

@inproceedings{fiber,
  title={Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone},
  author={Dou, Zi-Yi* and Kamath, Aishwarya* and Gan, Zhe* and Zhang, Pengchuan and Wang, Jianfeng and Li, Linjie and Liu, Zicheng and Liu, Ce and LeCun, Yann and Peng, Nanyun and Gao, Jianfeng and Wang, Lijuan},
  booktitle={NeurIPS},
  year={2022},
}
@inproceedings{sun2024generative,
  title={Generative multimodal models are in-context learners},
  author={Sun, Quan and Cui, Yufeng and Zhang, Xiaosong and Zhang, Fan and Yu, Qiying and Wang, Yueze and Rao, Yongming and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14398--14409},
  year={2024}
}

@article{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  journal={arXiv:2102.05918},
  year={2021}
}

@inproceedings{lit,
  title={LiT: Zero-Shot Transfer with Locked-image text Tuning},
  author={Xiaohua Zhai and Xiao Wang and Basil Mustafa and Andreas Steiner and Daniel Keysers and Alexander Kolesnikov and Lucas Beyer},
  booktitle={CVPR},
  year={2022},
}

@article{florence,
  title={Florence: A New Foundation Model for Computer Vision},
  author={Lu Yuan and Dongdong Chen and Yi-Ling Chen and Noel C. F. Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},
  journal={arXiv:2111.11432},
  year={2021}
}

@article{chinese_clip,
  title={Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese},
  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},
  journal={arXiv:2211.01335},
  year={2022}
}

@misc{pymupdf,
  author={Artifex Software},
  title={PyMuPDF},
  year={2015},
  url={https://github.com/pymupdf/PyMuPDF},
}

@misc{puppeteer,
  author={Google},
  title={Puppeteer},
  year={2023},
  url={https://github.com/puppeteer/puppeteer},
}

@inproceedings{young2014image_flickr30k,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  booktitle={ACL},
  year={2014},
}

@inproceedings{marino2019ok_okvqa,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{lu2022learn_scienceqa,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={CVPR},
  year={2018}
}

@article{masry2022chartqa,
  title={ChartQA: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv:2203.10244},
  year={2022}
}

@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={ECCV},
  year={2016},
}

@article{gupta2022grit,
  title={Grit: General robust image task benchmark},
  author={Gupta, Tanmay and Marten, Ryan and Kembhavi, Aniruddha and Hoiem, Derek},
  journal={arXiv:2204.13653},
  year={2022}
}

@article{xvlm,
  title={Multi-grained vision language pre-training: Aligning texts with visual concepts},
  author={Zeng, Yan and Zhang, Xinsong and Li, Hang},
  journal={arXiv:2111.08276},
  year={2021}
}

@article{x2vlm,
  title={X $\^{} 2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks},
  author={Zeng, Yan and Zhang, Xinsong and Li, Hang and Wang, Jiawei and Zhang, Jipeng and Zhou, Wangchunshu},
  journal={arXiv preprint arXiv:2211.12402},
  year={2022}
}

@inproceedings{rices,
  title={An empirical study of gpt-3 for few-shot knowledge-based vqa},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  booktitle={AAAI},
  year={2022}
}

@article{qwenvl,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv:2308.12966},
  year={2023}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv:2409.12191},
  year={2024}
}

@article{llava1.5,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv:2310.03744},
  year={2023}
}

@article{cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv:2311.03079},
  year={2023}
}

@misc{gpt4v,
    title = {GPT-4V(ision) System Card},
    url = {https://openai.com/research/gpt-4v-system-card},
    author = {OpenAI.},
    year = {2023}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv:2407.21783},
  year={2024}
}

@article{qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv:2309.16609},
  year={2023}
}

@article{gemini,
  title={Gemini: A family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv:2312.11805},
  year={2023}
}

@article{monkey,
  title={Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models},
  author={Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
  journal={arXiv:2311.06607},
  year={2023}
}

@article{otterhd,
  title={OtterHD: A High-Resolution Multi-modality Model},
  author={Li, Bo and Zhang, Peiyuan and Yang, Jingkang and Zhang, Yuanhan and Pu, Fanyi and Liu, Ziwei},
  journal={arXiv:2311.04219},
  year={2023}
}

@article{mplug-owl2,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv:2311.04257},
  year={2023}
}
@article{ye2024mplug,
  title={mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models},
  author={Ye, Jiabo and Xu, Haiyang and Liu, Haowei and Hu, Anwen and Yan, Ming and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2408.04840},
  year={2024}
}

@article{internlm-xcomposer,
  title={Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition},
  author={Zhang, Pan and Wang, Xiaoyi Dong Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Ding, Shuangrui and Zhang, Songyang and Duan, Haodong and Yan, Hang and others},
  journal={arXiv:2309.15112},
  year={2023}
}

@article{sharegpt4v,
  title={ShareGPT4V: Improving Large Multi-Modal Models with Better Captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv:2311.12793},
  year={2023}
}

@article{vlit,
  title={Vision-Language Instruction Tuning: A Review and Analysis},
  author={Li, Chen and Ge, Yixiao and Li, Dian and Shan, Ying},
  journal={arXiv:2311.08172},
  year={2023}
}

@article{dong2024internlm,
  title={InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Wei, Xilin and Zhang, Songyang and Duan, Haodong and Cao, Maosong and others},
  journal={arXiv preprint arXiv:2401.16420},
  year={2024}
}

@misc{yivl34b,
    title={Yi-VL-34B},
    url={https://huggingface.co/01-ai/Yi-VL-34B},
    author={01-ai},
    month={January},
    year={2024}
}

@misc{liu2024llava16,
    title={LLaVA-1.6: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-1-6/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@article{visionllm,
  title={VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks},
  author={Wen Wang and Zhe Chen and Xiaokang Chen and Jiannan Wu and Xizhou Zhu and Gang Zeng and Ping Luo and Tong Lu and Jie Zhou and Y. Qiao and Jifeng Dai},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.11175},
  url={https://api.semanticscholar.org/CorpusID:258762579}
}

@article{grounding_dino,
  title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},
  author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chun-yue Li and Jianwei Yang and Hang Su and Jun-Juan Zhu and Lei Zhang},
  journal={arXiv:2303.05499},
  year={2023},
}

@inproceedings{uninext,
  title={Universal Instance Perception as Object Discovery and Retrieval},
  author={B. Yan and Yi Jiang and Jiannan Wu and D. Wang and Ping Luo and Zehuan Yuan and Huchuan Lu},
  booktitle={CVPR},
  year={2023},
}

@article{minigpt-v2,
  title={MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning},
  author={Jun Chen and Deyao Zhu and Xiaoqian Shen and Xiang Li and Zechun Liu and Pengchuan Zhang and Raghuraman Krishnamoorthi and Vikas Chandra and Yunyang Xiong and Mohamed Elhoseiny},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.09478},
  url={https://api.semanticscholar.org/CorpusID:264146906}
}

@article{ferret,
  title={Ferret: Refer and Ground Anything Anywhere at Any Granularity},
  author={Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.07704},
  url={https://api.semanticscholar.org/CorpusID:263834718}
}

@article{ferretv2,
  title={Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models},
  author={Haotian Zhang and Haoxuan You and Philipp Dufter and Bowen Zhang and Chen Chen and Hong{-}You Chen and Tsu{-}Jui Fu and William Yang Wang and Shih{-}Fu Chang and Zhe Gan and Yinfei Yang},
  journal={arXiv:2404.07973},
  year={2024},
}

@article{sphinx,
  title={SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models},
  author={Ziyi Lin and Chris Liu and Renrui Zhang and Peng Gao and Longtian Qiu and Han Xiao and Han Qiu and Chen Lin and Wenqi Shao and Keqin Chen and Jiaming Han and Siyuan Huang and Yichi Zhang and Xuming He and Hongsheng Li and Yu Jiao Qiao},
  journal={arXiv:2311.07575},
  year={2023},
}

@article{internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@article{internvl1.5,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv:2404.16821},
  year={2024}
}

@misc{internvl2,
  title={InternVL2: Better than the Best—Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  year={2024},
  url={https://internvl.github.io/blog/2024-07-02-InternVL-2.0},
}

@article{minicpm-v,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv:2408.01800},
  year={2024}
}

@article{qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv:2407.10671},
  year={2024}
}

@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and others},
    journal = {arXiv:2412.15115},
    year    = {2024}
}

@inproceedings{mathvista,
  title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author={Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu and Chunyuan Li and Hannaneh Hajishirzi and Hao Cheng and Kai{-}Wei Chang and Michel Galley and Jianfeng Gao},
  booktitle={ICLR},
  year={2024}
}
@article{mathvision,
  title={Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset}, 
  author={Ke Wang and Junting Pan and Weikang Shi and Zimu Lu and Mingjie Zhan and Hongsheng Li},
  journal={arXiv:2402.14804},
  year={2024}
}

@inproceedings{zhang2024mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Qiao, Yu and others},
  booktitle={European Conference on Computer Vision},
  pages={169--186},
  year={2024},
  organization={Springer}
}

@article{chen2024mega,
  title={MEGA-Bench: Scaling multimodal evaluation to over 500 real-world tasks},
  author={Chen, Jiacheng and Liang, Tianhao and Siu, Sherman and Wang, Zhengqing and Wang, Kai and Wang, Yubo and Ni, Yuansheng and Zhu, Wang and Jiang, Ziyan and Lyu, Bohan and others},
  journal={arXiv preprint arXiv:2410.10563},
  year={2024}
}

@article{wang2024muirbench,
  title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding},
  author={Wang, Fei and Fu, Xingyu and Huang, James Y and Li, Zekun and Liu, Qin and Liu, Xiaogeng and Ma, Mingyu Derek and Xu, Nan and Zhou, Wenxuan and Zhang, Kai and others},
  journal={arXiv preprint arXiv:2406.09411},
  year={2024}
}

@article{agrawal2024pixtral,
  title={Pixtral 12B},
  author={Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Bout, Baptiste and Chaplot, Devendra and Chudnovsky, Jessica and Costa, Diogo and De Monicault, Baudouin and Garg, Saurabh and Gervet, Theophile and others},
  journal={arXiv preprint arXiv:2410.07073},
  year={2024}
}

@inproceedings{fu2024blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  booktitle={European Conference on Computer Vision},
  pages={148--166},
  year={2024},
  organization={Springer}
}

@article{wang2024allseeing_v2,
  title={The All-Seeing Project V2: Towards General Relation Comprehension of the Open World},
  author={Wang, Weiyun and Ren, Yiming and Luo, Haowen and Li, Tiantong and Yan, Chenxiang and Chen, Zhe and Wang, Wenhai and Li, Qingyun and Lu, Lewei and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2402.19474},
  year={2024}
}

@misc{pai-lingjun,
  title={PAI-Lingjun Intelligent Computing Service},
  author={Alibaba-Cloud},
  year={2024},
  url={https://www.alibabacloud.com/en/product/pai-lingjun}
}

@misc{cpfs,
  title={Cloud Parallel File Storage (CPFS)},
  author={Alibaba-Cloud},
  year={2024},
  url={https://www.alibabacloud.com/en/product/cpfs}
}

@misc{gpt4o,
  title={Hello GPT-4o},
  author={OpenAI},
  year={2024},
  url={https://openai.com/index/hello-gpt-4o}
}
@misc{sonnet3_5,
  title={Claude 3.5 Sonnet},
  author={Anthropic},
  year={2024},
  url={https://www.anthropic.com/news/claude-3-5-sonnet}
}

@misc{oss,
  title={Object Storage Service (OSS)},
  author={Alibaba-Cloud},
  year={2024},
  url={https://www.alibabacloud.com/en/product/object-storage-service}
}

@misc{ffmpeg,
  title={ffmpeg tool},
  author={FFmpeg-Developers},
  year={2024},
  url={http://ffmpeg.org/}
}

@inproceedings{pytorchddp,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  booktitle={VLDB},
  year={2020}
}

@inproceedings{alexnet,
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  title={ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle={NeurIPS},
  year={2012},
}

@article{megatron1,
  author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  journal={arXiv:1909.08053},
  year={2019},
}

@inproceedings{megatron2,
  author={Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
  title={Efficient large-scale language model training on {GPU} clusters using megatron-LM},
  booktitle={SC},
  year={2021},
}

@inproceedings{megatron3,
  author={Vijay Anand Korthikanti and Jared Casper and Sangkug Lym and Lawrence McAfee and Michael Andersch and Mohammad Shoeybi and Bryan Catanzaro},
  title={Reducing Activation Recomputation in Large Transformer Models},
  booktitle={MLSys},
  year={2023},
}

@inproceedings{gpipe,
  author={Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Dehao Chen and Mia Xu Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen},
  title={GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  booktitle={NeurIPS},
  year={2019},
}

@inproceedings{bfspp,
  author={Joel Lamy{-}Poirier},
  title={Breadth-First Pipeline Parallelism},
  booktitle={MLSys},
  year={2023},
}

@inproceedings{zero1,
  author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
  title={ZeRO: memory optimizations toward training trillion parameter models},
  booktitle={SC},
  year={2020},
}

@article{checkpointing,
  author={Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
  title={Training Deep Nets with Sublinear Memory Cost},
  journal={arXiv:1604.06174},
  year={2016},
}

@inproceedings{pytorch,
  author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas K{\"{o}}pf and Edward Z. Yang and Zachary DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  booktitle ={NeurIPS},
  year={2019},
}

@inproceedings{pytorch2,
  author={Jason Ansel and Edward Z. Yang and Horace He and Natalia Gimelshein and Animesh Jain and Michael Voznesensky and Bin Bao and Peter Bell and David Berard and Evgeni Burovski and Geeta Chauhan and Anjali Chourdia and Will Constable and Alban Desmaison and Zachary DeVito and Elias Ellison and Will Feng and Jiong Gong and Michael Gschwind and Brian Hirsh and Sherlock Huang and Kshiteej Kalambarkar and Laurent Kirsch and Michael Lazos and Mario Lezcano and Yanbo Liang and Jason Liang and Yinghai Lu and C. K. Luk and Bert Maher and Yunjie Pan and Christian Puhrsch and Matthias Reso and Mark Saroufim and Marcos Yukio Siraichi and Helen Suk and Shunting Zhang and Michael Suo and Phil Tillet and Xu Zhao and Eikan Wang and Keren Zhou and Richard Zou and Xiaodong Wang and Ajit Mathews and William Wen and Gregory Chanan and Peng Wu and Soumith Chintala},
  title={PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  booktitle={ASPLOS},
  year={2024},
}

@misc{cuda,
  title={CUDA},
  author={Nvidia},
  year={2024},
  url={https://developer.nvidia.com/cuda-toolkit}
}

@misc{apex,
  title={Apex},
  author={Nvidia},
  year={2024},
  url={https://github.com/NVIDIA/apex}
}

@inproceedings{flash1,
  author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R{\'{e}}},
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle={NeurIPS},
  year={2022},
}

@inproceedings{flash2,
  author={Tri Dao},
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  booktitle={ICLR},
  year={2024},
}

@article{flash3,
  author={Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  journal={arXiv:2407.08608},
  year={2024},
}

@article{tang2024mtvqa,
  title={MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering}, 
  author={Jingqun Tang and Qi Liu and Yongjie Ye and Jinghui Lu and Shu Wei and Chunhui Lin and Wanqing Li and Mohamad Fitri Faiz Bin Mahmood and Hao Feng and Zhen Zhao and Yanjie Wang and Yuliang Liu and Hao Liu and Xiang Bai and Can Huang},
  year={2024},
  journal={arXiv:2405.11985},
}

@article{liu2024ocrbenchhiddenmysteryocr,
  title={OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models}, 
  author={Yuliang Liu and Zhang Li and Mingxin Huang and Biao Yang and Wenwen Yu and Chunyuan Li and Xucheng Yin and Cheng-lin Liu and Lianwen Jin and Xiang Bai},
  year={2023},
  journal={arXiv:2305.07895},
}

@inproceedings{textvqa,
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  title={Towards VQA Models That Can Read},
  booktitle={CVPR},
  year={2019}
}

@article{layernorm,
  author={Lei Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  title={Layer Normalization},
  journal={arXiv:1607.06450},
  year={2016},
}

@inproceedings{rmsnorm,
  author={Biao Zhang and Rico Sennrich},
  title={Root Mean Square Layer Normalization},
  booktitle={NeurIPS},
  year={2019},
}

@inproceedings{adamw,
  author={Ilya Loshchilov and Frank Hutter},
  title={Decoupled Weight Decay Regularization},
  booktitle={ICLR},
  year={2019},
}

@inproceedings{lu2021inter,
 title={Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning},
 author={Lu, Pan and Gong, Ran and Jiang, Shibiao and Qiu, Liang and Huang, Siyuan and Liang, Xiaodan and Zhu, Song-Chun},
 booktitle={ACL},
 year={2021}
}

@misc{berkeley-function-calling-leaderboard,
  title={Berkeley Function Calling Leaderboard},
  author={Fanjia Yan and Huanzhi Mao and Charlie Cheng-Jie Ji and Tianjun Zhang and Shishir G. Patil and Ion Stoica and Joseph E. Gonzalez},
  year={2024},
  url={https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html},
}

@inproceedings{srinivasan2023nexusraven,
  title={Nexusraven: a commercially-permissive language model for function calling},
  author={Srinivasan, Venkat Krishna and Dong, Zhen and Zhu, Banghua and Yu, Brian and Mosk-Aoyama, Damon and Keutzer, Kurt and Jiao, Jiantao and Zhang, Jian},
  booktitle={NeurIPS Workshop},
  year={2023}
}

@article{chen2023t,
  title={T-eval: Evaluating the tool utilization capability step by step},
  author={Chen, Zehui and Du, Weihua and Zhang, Wenwei and Liu, Kuikun and Liu, Jiangning and Zheng, Miao and Zhuo, Jingming and Zhang, Songyang and Lin, Dahua and Chen, Kai and others},
  journal={arXiv:2312.14033},
  year={2023}
}

@misc{qwen-agent,
  title={Qwen-Agent Framework},
  author={Qwen Team, Alibaba Group},
  year={2024},
  url={https://github.com/QwenLM/Qwen-Agent}
}

@article{jiang2022vima,
  title={Vima: General robot manipulation with multimodal prompts},
  author={Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and Fei-Fei, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
  journal={arXiv:2210.03094},
  year={2022}
}

@article{huang2023instruct2act,
  title={Instruct2act: Mapping multi-modality instructions to robotic actions with large language model},
  author={Huang, Siyuan and Jiang, Zhengkai and Dong, Hao and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  journal={arXiv:2305.11176},
  year={2023}
}

@article{zhang2024android,
  title={Android in the zoo: Chain-of-action-thought for gui agents},
  author={Zhang, Jiwen and Wu, Jihao and Teng, Yihua and Liao, Minghui and Xu, Nuo and Xiao, Xiao and Wei, Zhongyu and Tang, Duyu},
  journal={arXiv:2403.02713},
  year={2024}
}

@inproceedings{rawles2024androidinthewild,
  title={Androidinthewild: A large-scale dataset for android device control},
  author={Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy},
  booktitle={NeurIPS},
  year={2024}
}

@article{lu2024gui,
  title={GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices},
  author={Lu, Quanfeng and Shao, Wenqi and Liu, Zitao and Meng, Fanqing and Li, Boxuan and Chen, Botong and Huang, Siyuan and Zhang, Kaipeng and Qiao, Yu and Luo, Ping},
  journal={arXiv:2406.08451},
  year={2024}
}

@article{zhai2024fine,
  title={Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning},
  author={Zhai, Yuexiang and Bai, Hao and Lin, Zipeng and Pan, Jiayi and Tong, Shengbang and Zhou, Yifei and Suhr, Alane and Xie, Saining and LeCun, Yann and Ma, Yi and others},
  journal={arXiv:2405.10292},
  year={2024}
}

@article{kolve2017ai2,
  title={Ai2-thor: An interactive 3d environment for visual ai},
  author={Kolve, Eric and Mottaghi, Roozbeh and Han, Winson and VanderBilt, Eli and Weihs, Luca and Herrasti, Alvaro and Deitke, Matt and Ehsani, Kiana and Gordon, Daniel and Zhu, Yuke and others},
  journal={arXiv:1712.05474},
  year={2017}
}

@inproceedings{shridhar2020alfred,
  title={Alfred: A benchmark for interpreting grounded instructions for everyday tasks},
  author={Shridhar, Mohit and Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan and Han, Winson and Mottaghi, Roozbeh and Zettlemoyer, Luke and Fox, Dieter},
  booktitle={CVPR},
  year={2020}
}

@article{inoue2022prompter,
  title={Prompter: Utilizing large language model prompting for a data efficient embodied instruction following},
  author={Inoue, Yuki and Ohashi, Hiroki},
  journal={arXiv:2211.03267},
  year={2022}
}

@article{lu2023thinkbot,
  title={Thinkbot: Embodied instruction following with thought chain reasoning},
  author={Lu, Guanxing and Wang, Ziwei and Liu, Changliu and Lu, Jiwen and Tang, Yansong},
  journal={arXiv:2312.07062},
  year={2023}
}

@article{rawles2024androidworld,
  title={AndroidWorld: A dynamic benchmarking environment for autonomous agents},
  author={Rawles, Christopher and Clinckemaillie, Sarah and Chang, Yifan and Waltz, Jonathan and Lau, Gabrielle and Fair, Marybeth and Li, Alice and Bishop, William and Li, Wei and Campbell-Ajala, Folawiyo and others},
  journal={arXiv:2405.14573},
  year={2024}
}

@article{shridhar2020alfworld,
  title={Alfworld: Aligning text and embodied environments for interactive learning},
  author={Shridhar, Mohit and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and Hausknecht, Matthew},
  journal={arXiv:2010.03768},
  year={2020}
}

@article{zhan2023you,
  title={You only look at screens: Multimodal chain-of-action agents},
  author={Zhan, Zhuosheng and Zhang, Aston},
  journal={arXiv:2309.11436},
  year={2023}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={ICCV},
  year={2023}
}


@inproceedings{vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{anderson2018vision,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{qi2020reverie,
  title={Reverie: Remote embodied visual referring expression in real indoor environments},
  author={Qi, Yuankai and Wu, Qi and Anderson, Peter and Wang, Xin and Wang, William Yang and Shen, Chunhua and Hengel, Anton van den},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chen2022think,
  title={Think global, act local: Dual-scale graph transformer for vision-and-language navigation},
  author={Chen, Shizhe and Guhur, Pierre-Louis and Tapaswi, Makarand and Schmid, Cordelia and Laptev, Ivan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{sigurdsson2023rrex,
  title={Rrex-bot: Remote referring expressions with a bag of tricks},
  author={Sigurdsson, Gunnar A and Thomason, Jesse and Sukhatme, Gaurav S and Piramuthu, Robinson},
  booktitle={IROS},
  year={2023},
}

@inproceedings{navit,
  title={Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution},
  author={Dehghani, Mostafa and Mustafa, Basil and Djolonga, Josip and Heek, Jonathan and Minderer, Matthias and Caron, Mathilde and Steiner, Andreas and Puigcerver, Joan and Geirhos, Robert and Alabdulmohsin, Ibrahim M and others},
  booktitle={NeurIPS},
  year={2024}
}


@misc{2drope,
  title={Transformer Upgrade Path: 4. Rotary Position Encoding for Two-Dimensional Positions},
  author={Su, Jianlin},
  year={2021},
  url={https://www.spaces.ac.cn/archives/8397}
}

@misc{ropetie,
  title={Transformer Upgrade Path: 17. Insights into Multimodal Positional Encoding},
  author={Su, Jianlin},
  year={2024},
  url={https://spaces.ac.cn/archives/10040}
}

@article{mme-realworld,
  title={MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?},
  author={Zhang, Yi-Fan and Zhang, Huanyu and Tian, Haochen and Fu, Chaoyou and Zhang, Shuangqing and Wu, Junfei and Li, Feng and Wang, Kun and Wen, Qingsong and Zhang, Zhang and others},
  journal={arXiv preprint arXiv:2408.13257},
  year={2024}
}

@inproceedings{i3d,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={CVPR},
  year={2017}
}

@article{mmmupro,
  title={MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark},
  author={Yue, Xiang and Zheng, Tianyu and Ni, Yuansheng and Wang, Yubo and Zhang, Kai and Tong, Shengbang and Sun, Yuxuan and Yin, Ming and Yu, Botao and Zhang, Ge and others},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}
@article{dong2024internlm,
  title={Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and others},
  journal={arXiv preprint arXiv:2404.06512},
  year={2024}
}

@article{zhang2024internlm,
  title={Internlm-xcomposer2. 5-omnilive: A comprehensive multimodal system for long-term streaming video and audio interactions},
  author={Zhang, Pan and Dong, Xiaoyi and Cao, Yuhang and Zang, Yuhang and Qian, Rui and Wei, Xilin and Chen, Lin and Li, Yifei and Niu, Junbo and Ding, Shuangrui and others},
  journal={arXiv preprint arXiv:2412.09596},
  year={2024}
}
@article{liang2025global,
  title={Global Semantic-Guided Sub-image Feature Weight Allocation in High-Resolution Large Vision-Language Models},
  author={Liang, Yuxuan and Li, Xu and Chen, Xiaolei and Chen, Haotian and Zheng, Yi and Lai, Chenghang and Li, Bin and Xue, Xiangyang},
  journal={arXiv preprint arXiv:2501.14276},
  year={2025}
}
@inproceedings{wang2023internimage,
  title={Internimage: Exploring large-scale vision foundation models with deformable convolutions},
  author={Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14408--14419},
  year={2023}
}

@article{wang2024emu3,
  title={Emu3: Next-token prediction is all you need},
  author={Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others},
  journal={arXiv preprint arXiv:2409.18869},
  year={2024}
}

@article{liu2024points,
  title={Points: Improving your vision-language model with affordable strategies},
  author={Liu, Yuan and Zhao, Zhongyin and Zhuang, Ziyuan and Tian, Le and Zhou, Xiao and Zhou, Jie},
  journal={arXiv preprint arXiv:2409.04828},
  year={2024}
}

@article{shi2024eagle,
  title={Eagle: Exploring the design space for multimodal llms with mixture of encoders},
  author={Shi, Min and Liu, Fuxiao and Wang, Shihao and Liao, Shijia and Radhakrishnan, Subhashree and Huang, De-An and Yin, Hongxu and Sapra, Karan and Yacoob, Yaser and Shi, Humphrey and others},
  journal={arXiv preprint arXiv:2408.15998},
  year={2024}
}
@article{li2024aria,
  title={Aria: An open multimodal native mixture-of-experts model},
  author={Li, Dongxu and Liu, Yudong and Wu, Haoning and Wang, Yue and Shen, Zhiqi and Qu, Bowen and Niu, Xinyao and Wang, Guoyin and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2410.05993},
  year={2024}
}
@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}
@article{li2024llava,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}
@article{tong2024cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}
@article{chen2024allava,
  title={Allava: Harnessing gpt4v-synthesized data for a lite vision-language model},
  author={Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou},
  journal={arXiv preprint arXiv:2402.11684},
  year={2024}
}
@article{liu2024mminstruct,
  title={Mminstruct: A high-quality multi-modal instruction tuning dataset with extensive diversity},
  author={Liu, Yangzhou and Cao, Yue and Gao, Zhangwei and Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Tian, Hao and Lu, Lewei and Zhu, Xizhou and Lu, Tong and others},
  journal={Science China Information Sciences},
  volume={67},
  number={12},
  pages={1--16},
  year={2024},
  publisher={Springer}
}
@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{guo2024mammoth,
  title={Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale},
  author={Guo, Jarvis and Zheng, Tuney and Bai, Yuelin and Li, Bo and Wang, Yubo and Zhu, King and Li, Yizhi and Neubig, Graham and Chen, Wenhu and Yue, Xiang},
  journal={arXiv preprint arXiv:2412.05237},
  year={2024}
}
@article{li2024baichuan,
  title={Baichuan-omni technical report},
  author={Li, Yadong and Sun, Haoze and Lin, Mingan and Li, Tianpeng and Dong, Guosheng and Zhang, Tao and Ding, Bowen and Song, Wei and Cheng, Zhenglin and Huo, Yuqi and others},
  journal={arXiv preprint arXiv:2410.08565},
  volume={3},
  number={7},
  year={2024}
}
@article{li2025baichuan,
  title={Baichuan-Omni-1.5 Technical Report},
  author={Li, Yadong and Liu, Jun and Zhang, Tao and Chen, Song and Li, Tianpeng and Li, Zehuan and Liu, Lijun and Ming, Lingfeng and Dong, Guosheng and Pan, Da and others},
  journal={arXiv preprint arXiv:2501.15368},
  year={2025}
}
@inproceedings{lee2024moai,
  title={Moai: Mixture of all intelligence for large language and vision models},
  author={Lee, Byung-Kwan and Park, Beomchan and Won Kim, Chae and Man Ro, Yong},
  booktitle={European Conference on Computer Vision},
  pages={273--302},
  year={2024},
  organization={Springer}
}
@article{ren2024grounding,
  title={Grounding DINO 1.5: Advance the" Edge" of Open-Set Object Detection},
  author={Ren, Tianhe and Jiang, Qing and Liu, Shilong and Zeng, Zhaoyang and Liu, Wenlong and Gao, Han and Huang, Hongjie and Ma, Zhengyu and Jiang, Xiaoke and Chen, Yihao and others},
  journal={arXiv preprint arXiv:2405.10300},
  year={2024}
}
@article{deitke2024molmo,
  title={Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
  author={Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
  journal={arXiv preprint arXiv:2409.17146},
  year={2024}
}


@article{hu2025video,
  title={Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos},
  author={Hu, Kairui and Wu, Penghao and Pu, Fanyi and Xiao, Wang and Zhang, Yuanhan and Yue, Xiang and Li, Bo and Liu, Ziwei},
  journal={arXiv preprint arXiv:2501.13826},
  year={2025}
}
@article{zhou2024mlvu,
  title={MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding},
  author={Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao, Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and Huang, Tiejun and Liu, Zheng},
  journal={arXiv preprint arXiv:2406.04264},
  year={2024}
}
@article{fang2024mmbench,
  title={MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2406.14515},
  year={2024}
}
@article{liu2024tempcompass,
  title={Tempcompass: Do video llms really understand videos?},
  author={Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2403.00476},
  year={2024}
}
@article{wang2024lvbench,
  title={Lvbench: An extreme long video understanding benchmark},
  author={Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Gu, Xiaotao and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2406.08035},
  year={2024}
}
@misc{zhao2025mmvu,
      title={MMVU: Measuring Expert-Level Multi-Discipline Video Understanding}, 
      author={Yilun Zhao and Lujing Xie and Haowei Zhang and Guo Gan and Yitao Long and Zhiyuan Hu and Tongyan Hu and Weiyuan Chen and Chuhan Li and Junyang Song and Zhijian Xu and Chengye Wang and Weifeng Pan and Ziyao Shangguan and Xiangru Tang and Zhenwen Liang and Yixin Liu and Chen Zhao and Arman Cohan},
      year={2025},
      eprint={2501.12380},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.12380}, 
}
@misc{wu2024longvideobench,
        title={LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding}, 
        author={Haoning Wu and Dongxu Li and Bei Chen and Junnan Li},
        year={2024},
        eprint={2407.15754},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2407.15754}, 
  }
@inproceedings{gao2017tall,
  title={Tall: Temporal activity localization via language query},
  author={Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5267--5275},
  year={2017}
}

% document and OCR benchmarks
@article{Kembhavi2016AI2D,
  title={A Diagram is Worth a Dozen Images},
  author={Aniruddha Kembhavi and Michael Salvato and Eric Kolve and Minjoon Seo and Hannaneh Hajishirzi and Ali Farhadi},
  journal={ArXiv},
  year={2016},
  volume={abs/1603.07396}
}


@article{Singh2019TextVQA,
  title={Towards VQA Models That Can Read},
  author={Amanpreet Singh and Vivek Natarajan and Meet Shah and Yu Jiang and Xinlei Chen and Dhruv Batra and Devi Parikh and Marcus Rohrbach},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={8309-8318}
}

@article{Mathew2020DocVQA,
  title={DocVQA: A Dataset for VQA on Document Images},
  author={Minesh Mathew and Dimosthenis Karatzas and R. Manmatha and C. V. Jawahar},
  journal={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2020},
  pages={2199-2208}
}

@article{Mathew2021InfographicVQA,
  title={InfographicVQA},
  author={Minesh Mathew and Viraj Bagal and Rub{\`e}n P{\'e}rez Tito and Dimosthenis Karatzas and Ernest Valveny and C.V. Jawahar},
  journal={2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2021},
  pages={2582-2591}
}

@article{Liu_2024_OCRBench,
    title={OCRBench: on the hidden mystery of OCR in large multimodal models},
    volume={67},
    ISSN={1869-1919},
    url={http://dx.doi.org/10.1007/s11432-024-4235-6},
    DOI={10.1007/s11432-024-4235-6},
    number={12},
    journal={Science China Information Sciences},
    publisher={Springer Science and Business Media LLC},
    author={Liu, Yuliang and Li, Zhang and Huang, Mingxin and Yang, Biao and Yu, Wenwen and Li, Chunyuan and Yin, Xu-Cheng and Liu, Cheng-Lin and Jin, Lianwen and Bai, Xiang},
    year={2024},
    month=dec }
  
@misc{fu2024ocrbenchv2improvedbenchmark,
    title={OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning}, 
    author={Ling Fu and Biao Yang and Zhebin Kuang and Jiajun Song and Yuzhe Li and Linghao Zhu and Qidi Luo and Xinyu Wang and Hao Lu and Mingxin Huang and Zhang Li and Guozhi Tang and Bin Shan and Chunhui Lin and Qi Liu and Binghong Wu and Hao Feng and Hao Liu and Can Huang and Jingqun Tang and Wei Chen and Lianwen Jin and Yuliang Liu and Xiang Bai},
    year={2024},
    eprint={2501.00321},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2501.00321}, 
}

@misc{yang2024ccocrcomprehensivechallengingocr,
      title={CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy}, 
      author={Zhibo Yang and Jun Tang and Zhaohai Li and Pengfei Wang and Jianqiang Wan and Humen Zhong and Xuejing Liu and Mingkun Yang and Peng Wang and Shuai Bai and LianWen Jin and Junyang Lin},
      year={2024},
      eprint={2412.02210},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.02210}, 
}

@misc{ouyang2024omnidocbenchbenchmarkingdiversepdf,
      title={OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations}, 
      author={Linke Ouyang and Yuan Qu and Hongbin Zhou and Jiawei Zhu and Rui Zhang and Qunshu Lin and Bin Wang and Zhiyuan Zhao and Man Jiang and Xiaomeng Zhao and Jin Shi and Fan Wu and Pei Chu and Minghao Liu and Zhenxiang Li and Chao Xu and Bo Zhang and Botian Shi and Zhongying Tu and Conghui He},
      year={2024},
      eprint={2412.07626},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.07626}, 
}

@article{wang2024charxiv,
  title={CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs},
  author={Wang, Zirui and Xia, Mengzhou and He, Luxi and Chen, Howard and Liu, Yitao and Zhu, Richard and Liang, Kaiqu and Wu, Xindi and Liu, Haotian and Malladi, Sadhika and Chevalier, Alexis and Arora, Sanjeev and Chen, Danqi},
  journal={arXiv preprint arXiv:2406.18521},
  year={2024}
}

@article{li2024seed2plus,
  title={SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension},
  author={Li, Bohao and Ge, Yuying and Chen, Yi and Ge, Yixiao and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2404.16790},
  year={2024}
}

@inproceedings{li2022grounded,
  title={Grounded language-image pre-training},
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10965--10975},
  year={2022}
}
@inproceedings{paiss2023teaching,
  title={Teaching clip to count to ten},
  author={Paiss, Roni and Ephrat, Ariel and Tov, Omer and Zada, Shiran and Mosseri, Inbar and Irani, Michal and Dekel, Tali},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3170--3180},
  year={2023}
}
@article{ye2024x,
  title={X-VILA: Cross-Modality Alignment for Large Language Model},
  author={Ye, Hanrong and Huang, De-An and Lu, Yao and Yu, Zhiding and Ping, Wei and Tao, Andrew and Kautz, Jan and Han, Song and Xu, Dan and Molchanov, Pavlo and others},
  journal={arXiv preprint arXiv:2405.19335},
  year={2024}
}
@article{xiao2023florence,
  title={Florence-2: Advancing a unified representation for a variety of vision tasks (2023)},
  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},
  journal={URL https://arxiv. org/abs/2311.06242},
  year={2023}
}
@article{zhang2024omg,
  title={Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding},
  author={Zhang, Tao and Li, Xiangtai and Fei, Hao and Yuan, Haobo and Wu, Shengqiong and Ji, Shunping and Loy, Chen Change and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2406.19389},
  year={2024}
}
@article{riquelme2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}
@article{li2024uni,
  title={Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts},
  author={Li, Yunxin and Jiang, Shenyuan and Hu, Baotian and Wang, Longyue and Zhong, Wanqi and Luo, Wenhan and Ma, Lin and Zhang, Min},
  journal={arXiv preprint arXiv:2405.11273},
  year={2024}
}
@article{wu2024deepseek,
  title={Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding},
  author={Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and others},
  journal={arXiv preprint arXiv:2412.10302},
  year={2024}
}
@article{DBLP:journals/corr/abs-2201-11903,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Ed H. Chi and
                  Quoc Le and
                  Denny Zhou},
  title        = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2201.11903},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.11903},
  eprinttype    = {arXiv},
  eprint       = {2201.11903},
  timestamp    = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2412-19437,
  author       = {DeepSeek{-}AI and
                  Aixin Liu and
                  Bei Feng and
                  Bing Xue and
                  Bingxuan Wang and
                  Bochao Wu and
                  Chengda Lu and
                  Chenggang Zhao and
                  Chengqi Deng and
                  Chenyu Zhang and
                  Chong Ruan and
                  Damai Dai and
                  Daya Guo and
                  Dejian Yang and
                  Deli Chen and
                  Dongjie Ji and
                  Erhang Li and
                  Fangyun Lin and
                  Fucong Dai and
                  Fuli Luo and
                  Guangbo Hao and
                  Guanting Chen and
                  Guowei Li and
                  H. Zhang and
                  Han Bao and
                  Hanwei Xu and
                  Haocheng Wang and
                  Haowei Zhang and
                  Honghui Ding and
                  Huajian Xin and
                  Huazuo Gao and
                  Hui Li and
                  Hui Qu and
                  J. L. Cai and
                  Jian Liang and
                  Jianzhong Guo and
                  Jiaqi Ni and
                  Jiashi Li and
                  Jiawei Wang and
                  Jin Chen and
                  Jingchang Chen and
                  Jingyang Yuan and
                  Junjie Qiu and
                  Junlong Li and
                  Junxiao Song and
                  Kai Dong and
                  Kai Hu and
                  Kaige Gao and
                  Kang Guan and
                  Kexin Huang and
                  Kuai Yu and
                  Lean Wang and
                  Lecong Zhang and
                  Lei Xu and
                  Leyi Xia and
                  Liang Zhao and
                  Litong Wang and
                  Liyue Zhang and
                  Meng Li and
                  Miaojun Wang and
                  Mingchuan Zhang and
                  Minghua Zhang and
                  Minghui Tang and
                  Mingming Li and
                  Ning Tian and
                  Panpan Huang and
                  Peiyi Wang and
                  Peng Zhang and
                  Qiancheng Wang and
                  Qihao Zhu and
                  Qinyu Chen and
                  Qiushi Du and
                  R. J. Chen and
                  R. L. Jin and
                  Ruiqi Ge and
                  Ruisong Zhang and
                  Ruizhe Pan and
                  Runji Wang and
                  Runxin Xu and
                  Ruoyu Zhang and
                  Ruyi Chen and
                  S. S. Li and
                  Shanghao Lu and
                  Shangyan Zhou and
                  Shanhuang Chen and
                  Shaoqing Wu and
                  Shengfeng Ye and
                  Shengfeng Ye and
                  Shirong Ma and
                  Shiyu Wang and
                  Shuang Zhou and
                  Shuiping Yu and
                  Shunfeng Zhou and
                  Shuting Pan and
                  T. Wang and
                  Tao Yun and
                  Tian Pei and
                  Tianyu Sun and
                  W. L. Xiao and
                  Wangding Zeng},
  title        = {DeepSeek-V3 Technical Report},
  journal      = {CoRR},
  volume       = {abs/2412.19437},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2412.19437},
  doi          = {10.48550/ARXIV.2412.19437},
  eprinttype    = {arXiv},
  eprint       = {2412.19437},
  timestamp    = {Sat, 25 Jan 2025 22:09:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2412-19437.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ghiasi2021simple,
  title={Simple copy-paste is a strong data augmentation method for instance segmentation},
  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2918--2928},
  year={2021}
}
@misc{minimax2025minimax01scalingfoundationmodels,
      title={MiniMax-01: Scaling Foundation Models with Lightning Attention}, 
      author={MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},
      year={2025},
      eprint={2501.08313},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.08313}, 
}

@article{cheng2024seeclick,
  title={Seeclick: Harnessing gui grounding for advanced visual gui agents},
  author={Cheng, Kanzhi and Sun, Qiushi and Chu, Yougang and Xu, Fangzhi and Li, Yantao and Zhang, Jianbing and Wu, Zhiyong},
  journal={arXiv preprint arXiv:2401.10935},
  year={2024}
}

@misc{screenspotpro,
  author    = {Kaixin Li and Ziyang Meng and Hongzhan Lin and Ziyang Luo and Yuchen Tian and Jing Ma and Zhiyong Huang and Tat-Seng Chua},
  title     = {ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use},
  year      = {2025},
  note      = {Preprint},
  url       = {https://likaixin2000.github.io/papers/ScreenSpot_Pro.pdf},
}

@article{li2024effects,
  title={On the Effects of Data Scale on Computer Control Agents},
  author={Li, Wei and Bishop, William and Li, Alice and Rawles, Chris and Campbell-Ajala, Folawiyo and Tyamagundlu, Divya and Riva, Oriana},
  journal={arXiv preprint arXiv:2406.03679},
  year={2024}
}


@article{xie2025osworld,
  title={Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments},
  author={Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Toh, Jing Hua and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={52040--52094},
  year={2025}
}

@misc{gemini2,
  title={Introducing Gemini 2.0: our new AI model for the agentic era},
  author={Google Deepmind},
  year={2024},
  url={https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/}
}

@misc{sonnet3_5_computer_use,
  title={Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku},
  author={Anthropic},
  year={2024},
  url={https://www.anthropic.com/news/3-5-models-and-computer-use}
}

@article{xu2024aguvis,
  title={Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction},
  author={Xu, Yiheng and Wang, Zekun and Wang, Junli and Lu, Dunjie and Xie, Tianbao and Saha, Amrita and Sahoo, Doyen and Yu, Tao and Xiong, Caiming},
  journal={arXiv preprint arXiv:2412.04454},
  year={2024}
}

@article{wang2025mobile,
  title={Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks},
  author={Wang, Zhenhailong and Xu, Haiyang and Wang, Junyang and Zhang, Xi and Yan, Ming and Zhang, Ji and Huang, Fei and Ji, Heng},
  journal={arXiv preprint arXiv:2501.11733},
  year={2025}
}

@article{wang2024mobile2,
  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},
  author={Wang, Junyang and Xu, Haiyang and Jia, Haitao and Zhang, Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2406.01014},
  year={2024}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}


@article{hoskin1996,
    title={The ``awful idea of accountability'': Inscribing people into the measurement of objects},
    author={Hoskin, Keith},
    year={1996},
    editor={Rolland Munro and Jan Mouritsen},
    journal={Accountability: Power, ethos and the technologies of managing}
}

@misc{Athene2024,
    title = {Athene-70B: Redefining the Boundaries of Post-Training for Open Models},
    url = {https://nexusflow.ai/blogs/athene},
    author = {Frick, Evan and Jin, Peter and Li, Tianle and Ganesan, Karthik and Zhang, Jian and Jiao, Jiantao and Zhu, Banghua},    
    month = {July},
    year = {2024}
}
@article{Wang2024HelpSteer2PreferenceCR,
  title={{HelpSteer2-Preference}: Complementing Ratings with Preferences},
  author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},
  journal={CoRR},
  year={2024},
  volume={abs/2410.01257}
}

@article{Adler2024Nemotron43T,
  title={Nemotron-4 {340B} Technical Report},
  author={Bo Adler and Niket Agarwal and Ashwath Aithal and Dong H. Anh and Pallab Bhattacharya and Annika Brundyn and Jared Casper and Bryan Catanzaro and Sharon Clay and Jonathan Cohen and Sirshak Das and Ayush Dattagupta and Olivier Delalleau and Leon Derczynski and Yi Dong and Daniel Egert and Ellie Evans and Aleksander Ficek and Denys Fridman and Shaona Ghosh and Boris Ginsburg and Igor Gitman and Tomasz Grzegorzek and Robert Hero and Jining Huang and Vibhu Jawa and Joseph Jennings and Aastha Jhunjhunwala and John Kamalu and Sadaf Khan and Oleksii Kuchaiev and Patrick LeGresley and Hui Li and Jiwei Liu and Zihan Liu and Eileen Peters Long and Ameya Mahabaleshwarkar and Somshubra Majumdar and James Maki and Miguel Martinez and Maer Rodrigues de Melo and Ivan Moshkov and Deepak Narayanan and Sean Narenthiran and Jesus Navarro and Phong Nguyen and Osvald Nitski and Vahid Noroozi and Guruprasad Nutheti and Christopher Parisien and Jupinder Parmar and Mostofa Patwary and Krzysztof Pawelec and Wei Ping and Shrimai Prabhumoye and Rajarshi Roy and Trisha Saar and Vasanth Rao Naik Sabavat and Sanjeev Satheesh and Jane Polak Scowcroft and Jason D. Sewall and Pavel Shamis and Gerald Shen and Mohammad Shoeybi and Dave Sizer and Misha Smelyanskiy and Felipe Soares and Makesh Narsimhan Sreedhar and Dan Su and Sandeep Subramanian and Shengyang Sun and Shubham Toshniwal and Hao Wang and Zhilin Wang and Jiaxuan You and Jiaqi Zeng and Jimmy Zhang and Jing Zhang and Vivienne Zhang and Yian Zhang and Chen Zhu},
  journal={CoRR},
  year={2024},
  volume={abs/2406.11704}
}
@article{Frick2024HowTE,
  title={How to Evaluate Reward Models for {RLHF}},
  author={Evan Frick and Tianle Li and Connor Chen and Wei-Lin Chiang and Anastasios Nikolas Angelopoulos and Jiantao Jiao and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
  journal={CoRR},
  year={2024},
  volume={abs/2410.14872}
}
@article{Zhou2024RMBCB,
  title={{RMB}: Comprehensively Benchmarking Reward Models in {LLM} Alignment},
  author={Enyu Zhou and Guodong Zheng and Bing Wang and Zhiheng Xi and Shihan Dou and Rong Bao and Wei Shen and Limao Xiong and Jessica Fan and Yurong Mou and Rui Zheng and Tao Gui and Qi Zhang and Xuanjing Huang},
  journal={CoRR},
  year={2024},
  volume={abs/2410.09893}
}
@article{Lambert2024RewardBenchER,
  title={{RewardBench}: Evaluating Reward Models for Language Modeling},
  author={Nathan Lambert and Valentina Pyatkin and Jacob Daniel Morrison and Lester James Validad Miranda and Bill Yuchen Lin and Khyathi Raghavi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hanna Hajishirzi},
  journal={CoRR},
  year={2024},
  volume={abs/2403.13787}
}

@article{palm,
  title={{PaLM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={CoRR},
  volume={abs/2204.02311},
  year={2022}
}

@article{yang2024evaluating,
  title={Evaluating and Aligning CodeLLMs on Human Preference},
  author={Yang, Jian and Yang, Jiaxi and Jin, Ke and Miao, Yibo and Zhang, Lei and Yang, Liqun and Cui, Zeyu and Zhang, Yichang and Hui, Binyuan and Lin, Junyang},
  journal={CoRR},
  volume={abs/2412.05210},
  year={2024}
}

@article{deepseekmath,
  author       = {Zhihong Shao and
                  Peiyi Wang and
                  Qihao Zhu and
                  Runxin Xu and
                  Junxiao Song and
                  Mingchuan Zhang and
                  Y. K. Li and
                  Y. Wu and
                  Daya Guo},
  title        = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.03300},
  year         = {2024}
}

@article{wang2024secrets,
  title={Secrets of {RLHF} in large language models Part {II}: Reward modeling},
  author={Wang, Binghai and Zheng, Rui and Chen, Lu and Liu, Yan and Dou, Shihan and Huang, Caishuang and Shen, Wei and Jin, Senjie and Zhou, Enyu and Shi, Chenyu and others},
  journal={CoRR},
  volume={abs/2401.06080},
  year={2024}
}

@article{dou2024multi,
  title={Multi-Programming Language Sandbox for LLMs},
  author={Dou, Shihan and Zhang, Jiazheng and Zang, Jianxiang and Tao, Yunbo and Jia, Haoxiang and Liu, Shichun and Yang, Yuming and Wu, Shenxi and Zhang, Shaoqing and Wu, Muling and others},
  journal={CoRR},
  volume={abs/2410.23074},
  year={2024}
}


@article{quan2024language,
  title={Language Models Can Self-Lengthen to Generate Long Texts},
  author={Quan, Shanghaoran and Tang, Tianyi and Yu, Bowen and Yang, An and Liu, Dayiheng and Gao, Bofei and Tu, Jianhong and Zhang, Yichang and Zhou, Jingren and Lin, Junyang},
  journal={CoRR},
  volume={abs/2410.23933},
  year={2024},
  archivePrefix={arXiv},
  eprint={2410.23933},
  primaryClass={cs.CL}
}


@article{xiang2024aligning,
  title={Aligning Large Language Models via Self-Steering Optimization},
  author={Xiang, Hao and Yu, Bowen and Lin, Hongyu and Lu, Keming and Lu, Yaojie and Han, Xianpei and Sun, Le and Zhou, Jingren and Lin, Junyang},
  journal={CoRR},
  volume={abs/2410.17131},
  year={2024}
}

@article{palm2,
  title={{PaLM} 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={CoRR},
  volume={abs/2305.10403},
  year={2023}
}

@article{qwen2,
  author       = {An Yang and
                  Baosong Yang and
                  Binyuan Hui and
                  Bo Zheng and
                  Bowen Yu and
                  Chang Zhou and
                  Chengpeng Li and
                  Chengyuan Li and
                  Dayiheng Liu and
                  Fei Huang and
                  Guanting Dong and
                  Haoran Wei and
                  Huan Lin and
                  Jialong Tang and
                  Jialin Wang and
                  Jian Yang and
                  Jianhong Tu and
                  Jianwei Zhang and
                  Jianxin Ma and
                  Jianxin Yang and
                  Jin Xu and
                  Jingren Zhou and
                  Jinze Bai and
                  Jinzheng He and
                  Junyang Lin and
                  Kai Dang and
                  Keming Lu and
                  Keqin Chen and
                  Kexin Yang and
                  Mei Li and
                  Mingfeng Xue and
                  Na Ni and
                  Pei Zhang and
                  Peng Wang and
                  Ru Peng and
                  Rui Men and
                  Ruize Gao and
                  Runji Lin and
                  Shijie Wang and
                  Shuai Bai and
                  Sinan Tan and
                  Tianhang Zhu and
                  Tianhao Li and
                  Tianyu Liu and
                  Wenbin Ge and
                  Xiaodong Deng and
                  Xiaohuan Zhou and
                  Xingzhang Ren and
                  Xinyu Zhang and
                  Xipin Wei and
                  Xuancheng Ren and
                  Xuejing Liu and
                  Yang Fan and
                  Yang Yao and
                  Yichang Zhang and
                  Yu Wan and
                  Yunfei Chu and
                  Yuqiong Liu and
                  Zeyu Cui and
                  Zhenru Zhang and
                  Zhifang Guo and
                  Zhihao Fan},
  title        = {Qwen2 Technical Report},
  journal      = {CoRR},
  volume       = {abs/2407.10671},
  year         = {2024}
}

@article{yuan2023scaling,
  author       = {Zheng Yuan and
                  Hongyi Yuan and
                  Chengpeng Li and
                  Guanting Dong and
                  Chuanqi Tan and
                  Chang Zhou},
  title        = {Scaling Relationship on Learning Mathematical Reasoning with Large
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2308.01825},
  year         = {2023}
}

@misc{qwq,
  title={{QwQ}: Reflect deeply on the boundaries of the unknown},
  author={{Qwen Team}},
  year={2024},
  url={https://qwenlm.github.io/blog/qwq-32b-preview/}
}

@inproceedings{rafailov2024direct,
  author       = {Rafael Rafailov and
                  Archit Sharma and
                  Eric Mitchell and
                  Christopher D. Manning and
                  Stefano Ermon and
                  Chelsea Finn},
  title        = {Direct Preference Optimization: Your Language Model is Secretly a
                  Reward Model},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{lu2024online,
  author       = {Keming Lu and
                  Bowen Yu and
                  Fei Huang and
                  Yang Fan and
                  Runji Lin and
                  Chang Zhou},
  title        = {Online Merging Optimizers for Boosting Rewards and Mitigating Tax
                  in Alignment},
  journal      = {CoRR},
  volume       = {abs/2405.17931},
  year         = {2024}
}

@article{bai2022constitutional,
author       = {Yuntao Bai and
                  Saurav Kadavath and
                  Sandipan Kundu and
                  Amanda Askell and
                  Jackson Kernion and
                  Andy Jones and
                  Anna Chen and
                  Anna Goldie and
                  Azalia Mirhoseini and
                  Cameron McKinnon and
                  Carol Chen and
                  Catherine Olsson and
                  Christopher Olah and
                  Danny Hernandez and
                  Dawn Drain and
                  Deep Ganguli and
                  Dustin Li and
                  Eli Tran{-}Johnson and
                  Ethan Perez and
                  Jamie Kerr and
                  Jared Mueller and
                  Jeffrey Ladish and
                  Joshua Landau and
                  Kamal Ndousse and
                  Kamile Lukosiute and
                  Liane Lovitt and
                  Michael Sellitto and
                  Nelson Elhage and
                  Nicholas Schiefer and
                  Noem{\'{\i}} Mercado and
                  Nova DasSarma and
                  Robert Lasenby and
                  Robin Larson and
                  Sam Ringer and
                  Scott Johnston and
                  Shauna Kravec and
                  Sheer El Showk and
                  Stanislav Fort and
                  Tamera Lanham and
                  Timothy Telleen{-}Lawton and
                  Tom Conerly and
                  Tom Henighan and
                  Tristan Hume and
                  Samuel R. Bowman and
                  Zac Hatfield{-}Dodds and
                  Ben Mann and
                  Dario Amodei and
                  Nicholas Joseph and
                  Sam McCandlish and
                  Tom Brown and
                  Jared Kaplan},
  title        = {Constitutional {AI}: Harmlessness from {AI} Feedback},
  journal      = {CoRR},
  volume       = {abs/2212.08073},
  year         = {2022}
}

@article{cao2024towards,
  author       = {Boxi Cao and
                  Keming Lu and
                  Xinyu Lu and
                  Jiawei Chen and
                  Mengjie Ren and
                  Hao Xiang and
                  Peilin Liu and
                  Yaojie Lu and
                  Ben He and
                  Xianpei Han and
                  Le Sun and
                  Hongyu Lin and
                  Bowen Yu},
  title        = {Towards Scalable Automated Alignment of {LLMs}: A Survey},
  journal      = {CoRR},
  volume       = {abs/2406.01252},
  year         = {2024}
}

@article{lu2024large,
  author       = {Keming Lu and
                  Bowen Yu and
                  Chang Zhou and
                  Jingren Zhou},
  title        = {Large Language Models are Superpositions of All Characters: Attaining
                  Arbitrary Role-play via Self-Alignment},
  journal      = {CoRR},
  volume       = {abs/2401.12474},
  year         = {2024}
}

@article{dong2024autoif,
  title={Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models},
  author={Dong, Guanting and Lu, Keming and Li, Chengpeng and Xia, Tingyu and Yu, Bowen and Zhou, Chang and Zhou, Jingren},
  journal={CoRR},
  volume={abs/2406.13542},
  year={2024}
}

@article{ext5,
  title={{ExT5}: Towards extreme multi-task scaling for transfer learning},
  author={Aribandi, Vamsi and Tay, Yi and Schuster, Tal and Rao, Jinfeng and Zheng, Huaixiu Steven and Mehta, Sanket Vaibhav and Zhuang, Honglei and Tran, Vinh Q and Bahri, Dara and Ni, Jianmo and others},
  journal={CoRR},
  volume={abs/2111.10952},
  year={2021}
}

@article{pmmeval,
  title={{P-MMEval}: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of {LLMs}},
  author={Zhang, Yidan and Deng, Boyi and Wan, Yu and Yang, Baosong and Wei, Haoran and Huang, Fei and Yu, Bowen and Lin, Junyang and Zhou, Jingren},
  journal={CoRR},
  volume={abs/2411.09116},
  year={2024}
}

@article{dong2023abilities,
  author       = {Guanting Dong and
                  Hongyi Yuan and
                  Keming Lu and
                  Chengpeng Li and
                  Mingfeng Xue and
                  Dayiheng Liu and
                  Wei Wang and
                  Zheng Yuan and
                  Chang Zhou and
                  Jingren Zhou},
  title        = {How Abilities in Large Language Models are Affected by Supervised
                  Fine-tuning Data Composition},
  journal      = {CoRR},
  volume       = {abs/2310.05492},
  year         = {2023}
}

@inproceedings{zhao2024tree,
  author       = {Yingxiu Zhao and
                  Bowen Yu and
                  Binyuan Hui and
                  Haiyang Yu and
                  Minghao Li and
                  Fei Huang and
                  Nevin L. Zhang and
                  Yongbin Li},
  title        = {{Tree-Instruct}: {A} Preliminary Study of the Intrinsic Relationship
                  between Complexity and Alignment},
  booktitle    = {{LREC/COLING}},
  pages        = {16776--16789},
  publisher    = {{ELRA} and {ICCL}},
  year         = {2024}
}

@inproceedings{lu2023instag,
  title={\#{InsTag}: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models},
  author={Lu, Keming and Yuan, Hongyi and Yuan, Zheng and Lin, Runji and Lin, Junyang and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  booktitle={{ICLR}},
  publisher    = {OpenReview.net},
  year={2024}
}


@article{polylm,
  title={{PolyLM}: An open source polyglot large language model},
  author={Wei, Xiangpeng and Wei, Haoran and Lin, Huan and Li, Tianhao and Zhang, Pei and Ren, Xingzhang and Li, Mei and Wan, Yu and Cao, Zhiwei and Xie, Binbin and others},
  journal={CoRR},
  volume={abs/2307.06018},
  year={2023}
}

@article{refineweb,
  title={The {RefinedWeb} dataset for {Falcon} {LLM}: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={CoRR},
  volume={abs/2306.01116},
  year={2023}
}

@misc{bard,
  title = {An important next step on our {AI} journey},
  author = {Google},
  url = {https://blog.google/technology/ai/bard-google-ai-search-updates/},
  year={2023}
}

@misc{opencompass,
  title = {{OpenCompass}: A universal evaluation platform for foundation models},
  author = {{OpenCompass Contributors}},
  url = {https://github.com/open-compass/opencompass},
  year={2023}
}


  title = {{ChatML}},
  author = {{OpenAI}},
  url = {https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md},
  year = {2022}
}

@misc{code_interpreter,
  title = {{ChatGPT plugins}},
  author = {{OpenAI}},
  url = {https://openai.com/blog/chatgpt-plugins},
  year = {2023}
}

@article{xlmr,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={CoRR},
  volume={abs/1911.02116},
  year={2019}
}

@inproceedings{singla2024dynamic,
  author       = {Somanshu Singla and
                  Zhen Wang and
                  Tianyang Liu and
                  Abdullah Ashfaq and
                  Zhiting Hu and
                  Eric P. Xing},
  title        = {Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment
                  of Language Models},
  booktitle    = {{EMNLP}},
  pages        = {21889--21909},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@misc{StableBeluga2,
  title = {{StableBeluga2}},
  author = {{Stability AI}},
  url = {https://huggingface.co/stabilityai/StableBeluga2},
  year = {2023}
}


@misc{qwen-tool-eval,
  title = {Evaluation Benchmark for Tool Usage through {ReAct} Prompting},
  author = {{Qwen Team, Alibaba Group}},
  url = {https:
//github.com/QwenLM/Qwen-7B/tree/main/eval},
  year = {2023}
}

@article{code_llama,
  title={Code {Llama}: Open foundation models for code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={CoRR},
  volume={abs/2308.12950},
  year={2023}
}

@misc{qwen-code-interpreter-eval,
  title = {Evaluation Benchmark for Code Intepreter},
  author = {{Qwen Team}},
  url = { https:
//github.com/QwenLM/Qwen-Agent/tree/main/benchmark},
  year = {2023}
}


@misc{claude,
  title = {Introducing {Claude}},
  author = {Anthropic},
  institution = {Anthropic},
  url = {https://www.anthropic.com/index/introducing-claude},
  year={2023}
}


@techreport{claude2,
  title = {Claude 2},
  author = {Anthropic},
  institution = {Anthropic},
  url = {https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf},
  year={2023}
}

@misc{langchain,
    title = {{LangChain}: Building applications with {LLMs} through composability},
    author = {{LangChain, Inc.}},
    url = {https://python.langchain.com/},
    year = {2023}
}

@article{rope,
  author       = {Jianlin Su and
                  Murtadha H. M. Ahmed and
                  Yu Lu and
                  Shengfeng Pan and
                  Wen Bo and
                  Yunfeng Liu},
  title        = {RoFormer: Enhanced {Transformer} with Rotary Position Embedding},
  journal      = {Neurocomputing},
  volume       = {568},
  pages        = {127063},
  year         = {2024}
}

@misc{qwen1.5,
    title = {Introducing {Qwen1.5}},
    url = {https://qwenlm.github.io/blog/qwen1.5/},
    author = {{Qwen Team}},
    year = {2024}
}

@inproceedings{bostrom2020byte,
    title = "Byte Pair Encoding is Suboptimal for Language Model Pretraining",
    author = "Bostrom, Kaj  and
      Durrett, Greg",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.414",
    doi = "10.18653/v1/2020.findings-emnlp.414",
    pages = "4617--4624",
    abstract = "The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE{'}s greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.",
}

@inproceedings{gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {NeurIPS},
  year         = {2020}
}

@article{llama,
  title={{LLaMA}: Open and efficient foundation language models},
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023}
}

@article{llama2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023}
}

@article{layer_norm,
  author       = {Lei Jimmy Ba and
                  Jamie Ryan Kiros and
                  Geoffrey E. Hinton},
  title        = {Layer Normalization},
  journal      = {CoRR},
  volume       = {abs/1607.06450},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.06450},
  eprinttype    = {arXiv},
  eprint       = {1607.06450},
  timestamp    = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{geglu,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gelu,
  author       = {Dan Hendrycks and
                  Kevin Gimpel},
  title        = {Bridging Nonlinearities and Stochastic Regularizers with {Gaussian}
                  Error Linear Units},
  journal      = {CoRR},
  volume       = {abs/1606.08415},
  year         = {2016},
  url          = {http://arxiv.org/abs/1606.08415},
  eprinttype    = {arXiv},
  eprint       = {1606.08415},
  timestamp    = {Mon, 13 Aug 2018 16:46:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HendrycksG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glu,
  author       = {Yann N. Dauphin and
                  Angela Fan and
                  Michael Auli and
                  David Grangier},
  title        = {Language Modeling with Gated Convolutional Networks},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {70},
  pages        = {933--941},
  publisher    = {{PMLR}},
  year         = {2017}
}

@article{noamglu,
  title={{GLU} variants improve transformer},
  author={Shazeer, Noam},
  journal={CoRR},
  volume={abs/2002.05202},
  year={2020}
}

@article{swish,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={CoRR},
  volume={abs/1710.05941},
  year={2017}
}

@article{gpt4,
  title={{GPT4} technical report},
  author={{OpenAI}},
  journal={CoRR},
  volume={abs/2303.08774},
  year={2023}
}

@misc{tiktoken,
   title = {{tiktoken}: A fast {BPE} tokeniser for use with {OpenAI}'s models},
   author = {Shantanu Jain},
   institution = {OpenAI},
   year = {2022},
   url = {https://github.com/openai/tiktoken/}
}

@inproceedings{flashattn,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Stefano Ermon and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {{FlashAttention}: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/DaoFERR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{transformer,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention is All you Need},
  booktitle    = {{NIPS}},
  pages        = {5998--6008},
  year         = {2017}
}

@article{roberta,
  title={{RoBERTa}: A robustly optimized {BERT} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={CoRR},
  volume={abs/1907.11692},
  year={2019}
}



@inproceedings{flan,
  author       = {Jason Wei and
                  Maarten Bosma and
                  Vincent Y. Zhao and
                  Kelvin Guu and
                  Adams Wei Yu and
                  Brian Lester and
                  Nan Du and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {Finetuned Language Models are Zero-Shot Learners},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=gEZrGCozdqR},
  timestamp    = {Wed, 16 Aug 2023 16:10:28 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/WeiBZGYLDDL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{flanv2,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={CoRR},
  volume={abs/2210.11416},
  year={2022}
}

@article{flan-collection,
  title={The {Flan} collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={CoRR},
  volume={abs/2301.13688},
  year={2023}
}

@misc{codeqwen1.5,
    title = {Code with CodeQwen1.5},
    url = {https://qwenlm.github.io/blog/codeqwen1.5/},
    author = {{Qwen Team}},
    month = {April},
    year = {2024}
}

@article{cai,
  title={Constitutional {AI}: Harmlessness from {AI} feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={CoRR},
  volume={abs/2212.08073},
  year={2022}
}

@inproceedings{rlhf,
  author       = {Paul F. Christiano and
                  Jan Leike and
                  Tom B. Brown and
                  Miljan Martic and
                  Shane Legg and
                  Dario Amodei},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Deep Reinforcement Learning from Human Preferences},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {4299--4307},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/ChristianoLBMLA17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{red-teaming,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={CoRR},
  volume={abs/2209.07858},
  year={2022}
}

@misc{transformers-agents,
    title = {Transformers Agents},
    author = {{Hugging Face}},
    url = {https://huggingface.co/docs/transformers/transformers_agents}, 
    year = {2023}
}


@misc{qwen1.5moe,
    title = {{Qwen1.5-MoE}: Matching {7B} Model Performance with 1/3 Activated Parameters},
    url = {https://qwenlm.github.io/blog/qwen-moe/},
    author = {{Qwen Team}},
    year = {2024}
}

@article{thestack,
  author       = {Denis Kocetkov and
                  Raymond Li and
                  Loubna Ben Allal and
                  Jia Li and
                  Chenghao Mou and
                  Carlos Mu{\~{n}}oz Ferrandis and
                  Yacine Jernite and
                  Margaret Mitchell and
                  Sean Hughes and
                  Thomas Wolf and
                  Dzmitry Bahdanau and
                  Leandro von Werra and
                  Harm de Vries},
  title        = {The Stack: 3 {TB} of permissively licensed source code},
  journal      = {CoRR},
  volume       = {abs/2211.15533},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.15533},
  doi          = {10.48550/arXiv.2211.15533},
  eprinttype    = {arXiv},
  eprint       = {2211.15533},
  timestamp    = {Tue, 29 Nov 2022 17:41:18 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-15533.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{codeparrot-github-clean,
title = {{GitHub} Code Dataset Cleaned},
author = {{CodeParrot}},
url = {https://huggingface.co/datasets/codeparrot/github-code-clean},
year = {2022}
}

@article{starcoder,
  author       = {Raymond Li and
                  Loubna Ben Allal and
                  Yangtian Zi and
                  Niklas Muennighoff and
                  Denis Kocetkov and
                  Chenghao Mou and
                  Marc Marone and
                  Christopher Akiki and
                  Jia Li and
                  Jenny Chim and
                  Qian Liu and
                  Evgenii Zheltonozhskii and
                  Terry Yue Zhuo and
                  Thomas Wang and
                  Olivier Dehaene and
                  Mishig Davaadorj and
                  Joel Lamy{-}Poirier and
                  Jo{\~{a}}o Monteiro and
                  Oleh Shliazhko and
                  Nicolas Gontier and
                  Nicholas Meade and
                  Armel Zebaze and
                  Ming{-}Ho Yee and
                  Logesh Kumar Umapathi and
                  Jian Zhu and
                  Benjamin Lipkin and
                  Muhtasham Oblokulov and
                  Zhiruo Wang and
                  Rudra Murthy V and
                  Jason Stillerman and
                  Siva Sankalp Patel and
                  Dmitry Abulkhanov and
                  Marco Zocca and
                  Manan Dey and
                  Zhihan Zhang and
                  Nour Moustafa{-}Fahmy and
                  Urvashi Bhattacharyya and
                  Wenhao Yu and
                  Swayam Singh and
                  Sasha Luccioni and
                  Paulo Villegas and
                  Maxim Kunakov and
                  Fedor Zhdanov and
                  Manuel Romero and
                  Tony Lee and
                  Nadav Timor and
                  Jennifer Ding and
                  Claire Schlesinger and
                  Hailey Schoelkopf and
                  Jan Ebert and
                  Tri Dao and
                  Mayank Mishra and
                  Alex Gu and
                  Jennifer Robinson and
                  Carolyn Jane Anderson and
                  Brendan Dolan{-}Gavitt and
                  Danish Contractor and
                  Siva Reddy and
                  Daniel Fried and
                  Dzmitry Bahdanau and
                  Yacine Jernite and
                  Carlos Mu{\~{n}}oz Ferrandis and
                  Sean Hughes and
                  Thomas Wolf and
                  Arjun Guha and
                  Leandro von Werra and
                  Harm de Vries},
  title        = {{StarCoder}: May the source be with you!},
  journal      = {CoRR},
  volume       = {abs/2305.06161},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.06161},
  doi          = {10.48550/arXiv.2305.06161},
  eprinttype    = {arXiv},
  eprint       = {2305.06161},
  timestamp    = {Fri, 23 Jun 2023 22:30:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-06161.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{generalist,
  author       = {Scott E. Reed and
                  Konrad Zolna and
                  Emilio Parisotto and
                  Sergio G{\'{o}}mez Colmenarejo and
                  Alexander Novikov and
                  Gabriel Barth{-}Maron and
                  Mai Gimenez and
                  Yury Sulsky and
                  Jackie Kay and
                  Jost Tobias Springenberg and
                  Tom Eccles and
                  Jake Bruce and
                  Ali Razavi and
                  Ashley Edwards and
                  Nicolas Heess and
                  Yutian Chen and
                  Raia Hadsell and
                  Oriol Vinyals and
                  Mahyar Bordbar and
                  Nando de Freitas},
  title        = {A Generalist Agent},
  journal      = {Trans. Mach. Learn. Res.},
  volume       = {2022},
  year         = {2022},
  url          = {https://openreview.net/forum?id=1ikK0kHjvj},
  timestamp    = {Fri, 19 May 2023 11:20:41 +0200},
  biburl       = {https://dblp.org/rec/journals/tmlr/ReedZPCNBGSKSEBREHCHVBF22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={{Gemma Team} and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={CoRR},
  volume={abs/2408.00118},
  year={2024}
}

@article{mmluredux,
  title={Are We Done with MMLU?},
  author={Gema, Aryo Pradipta and Leang, Joshua Ong Jun and Hong, Giwon and Devoto, Alessio and Mancino, Alberto Carlo Maria and Saxena, Rohit and He, Xuanli and Zhao, Yu and Du, Xiaotang and Madani, Mohammad Reza Ghasemi and others},
  journal={CoRR},
  volume={abs/2406.04127},
  year={2024}
}

@misc{chatgpt,
    title = {Introducing {ChatGPT}} ,
    author = {{OpenAI}},
    url = {https://openai.com/index/chatgpt/},
    year = {2022}
}

@inproceedings{instructgpt,
  author       = {Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe},
  title        = {Training language models to follow instructions with human feedback},
  booktitle    = {NeurIPS},
  year         = {2022},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/Ouyang0JAWMZASR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{o1,
    title={Learning to Reason with {LLMs}},
    author={{OpenAI}},
    url={https://openai.com/index/learning-to-reason-with-llms/},
    year={2024}
}

@inproceedings{wolfe2024laboratory,
  author       = {Robert Wolfe and
                  Isaac Slaughter and
                  Bin Han and
                  Bingbing Wen and
                  Yiwei Yang and
                  Lucas Rosenblatt and
                  Bernease Herman and
                  Eva Maxfield Brown and
                  Zening Qu and
                  Nic Weber and
                  Bill Howe},
  title        = {Laboratory-Scale {AI:} Open-Weight Models are Competitive with {ChatGPT}
                  Even in Low-Resource Settings},
  booktitle    = {FAccT},
  pages        = {1199--1210},
  publisher    = {{ACM}},
  year         = {2024}
}

@inproceedings{kapoor2024societal,
  author       = {Sayash Kapoor and
                  Rishi Bommasani and
                  Kevin Klyman and
                  Shayne Longpre and
                  Ashwin Ramaswami and
                  Peter Cihon and
                  Aspen K. Hopkins and
                  Kevin Bankston and
                  Stella Biderman and
                  Miranda Bogen and
                  Rumman Chowdhury and
                  Alex Engler and
                  Peter Henderson and
                  Yacine Jernite and
                  Seth Lazar and
                  Stefano Maffulli and
                  Alondra Nelson and
                  Joelle Pineau and
                  Aviya Skowron and
                  Dawn Song and
                  Victor Storchan and
                  Daniel Zhang and
                  Daniel E. Ho and
                  Percy Liang and
                  Arvind Narayanan},
  title        = {Position: On the Societal Impact of Open Foundation Models},
  booktitle    = {{ICML}},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@article{qwen2.5coder,
  title={{Qwen2.5-Coder} technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={CoRR},
  volume={abs/2409.12186},
  year={2024}
}


@misc{qwen2math,
  title={{Introducing Qwen2-Math}},
  author={{Qwen Team}},
  year={2024},
  url={https://qwenlm.github.io/blog/qwen2-math/}
}

@article{qwen2.5math,
  title={{Qwen2.5-Math} technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={CoRR},
  volume={abs/2409.12122},
  year={2024}
}

@article{kaplanscaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={CoRR},
  volume={abs/2001.08361},
  year={2020}
}

@article{chinchilla,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={CoRR},
  volume={abs/2203.15556},
  year={2022}
}

@article{scaling_rush,
  title={Scaling Data-Constrained Language Models},
  author={Muennighoff, Niklas and Rush, Alexander M and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  journal={CoRR},
  volume={abs/2305.16264},
  year={2023}
}


@inproceedings{mmlu,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andy Zou and
                  Mantas Mazeika and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Massive Multitask Language Understanding},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@inproceedings{bbh,
  author       = {Mirac Suzgun and
                  Nathan Scales and
                  Nathanael Sch{\"{a}}rli and
                  Sebastian Gehrmann and
                  Yi Tay and
                  Hyung Won Chung and
                  Aakanksha Chowdhery and
                  Quoc V. Le and
                  Ed H. Chi and
                  Denny Zhou and
                  Jason Wei},
  title        = {Challenging {BIG-Bench} Tasks and Whether Chain-of-Thought Can Solve
                  Them},
  booktitle    = {{ACL} (Findings)},
  pages        = {13003--13051},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@article{mbpp,
  author       = {Jacob Austin and
                  Augustus Odena and
                  Maxwell I. Nye and
                  Maarten Bosma and
                  Henryk Michalewski and
                  David Dohan and
                  Ellen Jiang and
                  Carrie J. Cai and
                  Michael Terry and
                  Quoc V. Le and
                  Charles Sutton},
  title        = {Program Synthesis with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2108.07732},
  year         = {2021}
}

@article{cmmlu,
  author       = {Haonan Li and
                  Yixuan Zhang and
                  Fajri Koto and
                  Yifei Yang and
                  Hai Zhao and
                  Yeyun Gong and
                  Nan Duan and
                  Timothy Baldwin},
  title        = {{CMMLU}: Measuring massive multitask language understanding in {Chinese}},
  journal      = {CoRR},
  volume       = {abs/2306.09212},
  year         = {2023}
}

@inproceedings{ceval,
  author       = {Yuzhen Huang and
                  Yuzhuo Bai and
                  Zhihao Zhu and
                  Junlei Zhang and
                  Jinghan Zhang and
                  Tangjun Su and
                  Junteng Liu and
                  Chuancheng Lv and
                  Yikai Zhang and
                  Jiayi Lei and
                  Yao Fu and
                  Maosong Sun and
                  Junxian He},
  title        = {{C-Eval}: A Multi-Level Multi-Discipline Chinese Evaluation Suite
                  for Foundation Models},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{gsm8k,
  author       = {Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
  title        = {Training Verifiers to Solve Math Word Problems},
  journal      = {CoRR},
  volume       = {abs/2110.14168},
  year         = {2021}
}


@article{pile,
  title={The {Pile}: An {800GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={CoRR},
  volume={abs/2101.00027},
  year={2020}
}



@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}



@article{instructblip,
  title={{InstructBLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Meng, Anthony and  Tiong, Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
  journal={CoRR},
  volume={abs/2305.06500},
  year={2023}
}


@article{toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={CoRR},
  volume={abs/2302.04761},
  year={2023}
}

@article{megatron,
  title={{Megatron-LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={CoRR},
  volume={abs/1909.08053},
  year={2019}
}

@article{bloom,
  title={{BLOOM}: A {176B}-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={CoRR},
  volume={abs/2211.05100},
  year={2022}
}

@inproceedings{xwinograd,
  author       = {Niklas Muennighoff and
                  Thomas Wang and
                  Lintang Sutawika and
                  Adam Roberts and
                  Stella Biderman and
                  Teven Le Scao and
                  M. Saiful Bari and
                  Sheng Shen and
                  Zheng Xin Yong and
                  Hailey Schoelkopf and
                  Xiangru Tang and
                  Dragomir Radev and
                  Alham Fikri Aji and
                  Khalid Almubarak and
                  Samuel Albanie and
                  Zaid Alyafeai and
                  Albert Webson and
                  Edward Raff and
                  Colin Raffel},
  title        = {Crosslingual Generalization through Multitask Finetuning},
  booktitle    = {{ACL} {(1)}},
  pages        = {15991--16111},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@article{opt,
  title={{OPT}: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={CoRR},
  volume={abs/2205.01068},
  year={2022}
}

@article{glm-130b,
  title={{GLM-130B}: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={CoRR},
  volume={abs/2210.02414},
  year={2022}
}

@article{glm,
  title={{GLM}: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  journal={CoRR},
  volume={abs/2103.10360},
  year={2021}
}

@inproceedings{glam,
  title={{GLaM}: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@article{gshard,
  title={{GShard}: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={CoRR},
  volume={abs/2006.16668},
  year={2020}
}

@article{switch_transformer,
  author       = {William Fedus and
                  Barret Zoph and
                  Noam Shazeer},
  title        = {Switch Transformers: Scaling to Trillion Parameter Models with Simple
                  and Efficient Sparsity},
  journal      = {J. Mach. Learn. Res.},
  volume       = {23},
  pages        = {120:1--120:39},
  year         = {2022}
}

@article{stmoe,
  title={{ST-MoE}: Designing stable and transferable sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={CoRR},
   volume={abs/2202.08906},
  year={2022}
}

@article{gpt-neo,
  title={{GPT-NeoX-20B}: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={CoRR},
  volume={abs/2204.06745},
  year={2022}
}

@article{gopher,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={CoRR},
  volume={abs/2112.11446},
  year={2021}
}

@article{foundation_models,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={CoRR},
  volume={abs/2108.07258},
  year={2021}
}

@misc{mpt,
    title={{MPT-30B}: Raising the bar for open-source foundation models},
    author={{Mosaic ML}},
    url={https://www.mosaicml.com/blog/mpt-30b},
    year={2023},
}

@article{falcon,
  author       = {Ebtesam Almazrouei and
                  Hamza Alobeidli and
                  Abdulaziz Alshamsi and
                  Alessandro Cappelli and
                  Ruxandra Cojocaru and
                  M{\'{e}}rouane Debbah and
                  {\'{E}}tienne Goffinet and
                  Daniel Hesslow and
                  Julien Launay and
                  Quentin Malartic and
                  Daniele Mazzotta and
                  Badreddine Noune and
                  Baptiste Pannier and
                  Guilherme Penedo},
  title        = {The {Falcon} Series of Open Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.16867},
  year         = {2023}
}

@article{t0,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={CoRR},
  volume={abs/2110.08207},
  year={2021}
}



@article{mplug-owl,
  title={{mPLUG-Owl}: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={CoRR},
  volume={abs/2304.14178},
  year={2023}
}

@techreport{baichuan2,
    author = {Aiyuan Yang and Bin Xiao and Bingning Wang and Borong Zhang and Chao Yin and Chenxu Lv and Da Pan and Dian Wang and Dong Yan and Fan Yang and Fei Deng and Feng Wang and Feng Liu and Guangwei Ai and Guosheng Dong and Haizhou Zhao and Hang Xu and Haoze Sun and Hongda Zhang and Hui Liu and Jiaming Ji and Jian Xie and Juntao Dai and Kun Fang and Lei Su and Liang Song and Lifeng Liu and Liyun Ru and Luyao Ma and Mang Wang and Mickel Liu and MingAn Lin and Nuolan Nie and Peidong Guo and Ruiyang Sun and Tao Zhang and Tianpeng Li and Tianyu Li and Wei Cheng and Weipeng Chen and Xiangrong Zeng and Xiaochuan Wang and Xiaoxi Chen and Xin Men and Xin Yu and Xuehai Pan and Yanjun Shen and Yiding Wang and Yiyu Li and Youxin Jiang and Yuchen Gao and Yupeng Zhang and Zenan Zhou and Zhiying Wu},
    title = {Baichuan 2: Open Large-scale Language Models},
    institution = {Baichuan Inc.},
    year = {2023},
    url = {https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf}
}

@article{self_align,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={CoRR},
  volume={abs/2305.03047},
  year={2023}
}

@article{wizardlm,
  title={{WizardLM}: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={CoRR},
  volume={abs/2304.12244},
  year={2023}
}

@article{expertprompting,
  title={{ExpertPrompting}: Instructing Large Language Models to be Distinguished Experts},
  author={Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
  journal={CoRR},
  volume={abs/2305.14688},
  year={2023}
}

@article{ultrachat,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={CoRR},
  volume={abs/2305.14233},
  year={2023}
}

@article{baize,
  title={{BaiZe}: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={CoRR},
  volume={abs/2304.01196},
  year={2023}
}

@article{phoenix,
  title={Phoenix: Democratizing {ChatGPT} across languages},
  author={Chen, Zhihong and Jiang, Feng and Chen, Junying and Wang, Tiannan and Yu, Fei and Chen, Guiming and Zhang, Hongbo and Liang, Juhao and Zhang, Chen and Zhang, Zhiyi and others},
  journal={CoRR},
  volume={abs/2304.10453},
  year={2023}
}

@misc{dolly,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free {Dolly}: Introducing the World's First Truly Open Instruction-Tuned {LLM}},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}

@article{orca,
  title={Orca: Progressive learning from complex explanation traces of {GPT-4}},
  author={Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
  journal={CoRR},
  volume={abs/2306.02707},
  year={2023}
}

@misc{moss,
  title={{MOSS}: Training Conversational Language Models from Synthetic Data}, 
  author={Tianxiang Sun and Xiaotian Zhang and Zhengfu He and Peng Li and Qinyuan Cheng and Hang Yan and Xiangyang Liu and Yunfan Shao and Qiong Tang and Xingjian Zhao and Ke Chen and Yining Zheng and Zhejian Zhou and Ruixiao Li and Jun Zhan and Yunhua Zhou and Linyang Li and Xiaogui Yang and Lingling Wu and Zhangyue Yin and Xuanjing Huang and Xipeng Qiu},
  year={2023}
}



@article{huggingface,
  title={{HuggingFace}'s transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={CoRR},
  volume={abs/1910.03771},
  year={2019}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={CoRR},
  volume={abs/1412.6980},
  year={2014}
}

@article{adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={CoRR},
  volume={abs/1711.05101},
  year={2017}
}

@inproceedings{ul2,
  title={{UL2}: Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Steven and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@misc{internlm,
    title={{InternLM}: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={{InternLM Team}},
    url = {https://github.com/InternLM/InternLM},
    year={2023}
}

@misc{baichuan7b,
    title={Baichuan-{7B}: A large-scale {7B} pretraining language model developed by {BaiChuan-Inc}},
    author={Baichuan Inc.},
    url = {https://github.com/baichuan-inc/Baichuan-7B},
    year={2023}
}

@misc{chatglm2,
    title={{ChatGLM2-6B}: An Open Bilingual Chat {LLM}},
    author={{ChatGLM2 Team}},
    url = {https://github.com/THUDM/ChatGLM2-6B},
    year={2023}
}

@misc{xverse,
    title={{XVERSE-13B}: A multilingual large language model developed by {XVERSE Technology Inc.}},
    author={XVERSE Technology Inc.},
    url = {https://github.com/xverse-ai/XVERSE-13B},
    year={2023}
}

@misc{baichuan13b,
    title={Baichuan-{13B}: A {13B} large language model developed by Baichuan Intelligent Technology},
    author={Baichuan Inc.},
    url = {https://github.com/baichuan-inc/Baichuan-13B},
    year={2023}
}

@article{upalm,
  title={Transcending scaling laws with 0.1\% extra compute},
  author={Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q and So, David R and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and others},
  journal={CoRR},
  volume={abs/2210.11399},
  year={2022}
}

@techreport{gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  institution={{OpenAI}}
}

@article{qwenvl,
  author       = {Jinze Bai and
                  Shuai Bai and
                  Shusheng Yang and
                  Shijie Wang and
                  Sinan Tan and
                  Peng Wang and
                  Junyang Lin and
                  Chang Zhou and
                  Jingren Zhou},
  title        = {{Qwen-VL}: A Frontier Large Vision-Language Model with Versatile Abilities},
  journal      = {CoRR},
  volume       = {abs/2308.12966},
  year         = {2023}
}

@article{ofasys,
  author       = {Jinze Bai and
                  Rui Men and
                  Hao Yang and
                  Xuancheng Ren and
                  Kai Dang and
                  Yichang Zhang and
                  Xiaohuan Zhou and
                  Peng Wang and
                  Sinan Tan and
                  An Yang andf
                  Zeyu Cui and
                  Yu Han and
                  Shuai Bai and
                  Wenbin Ge and
                  Jianxin Ma and
                  Junyang Lin and
                  Jingren Zhou and
                  Chang Zhou},
  title        = {{OFASys}: {A} Multi-Modal Multi-Task Learning System for Building Generalist
                  Models},
  journal      = {CoRR},
  volume       = {abs/2212.04408},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.04408},
  doi          = {10.48550/arXiv.2212.04408},
  eprinttype    = {arXiv},
  eprint       = {2212.04408},
  timestamp    = {Mon, 02 Jan 2023 15:09:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-04408.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{qwen1.5_110,
    title = {{Qwen1.5-110B}: The First {100B+} Model of the {Qwen1.5} Series},
    author = {{Qwen Team}},
    url = {https://qwenlm.github.io/blog/qwen1.5-110b/},
    year = 2024
}

@misc{rft,
      title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models}, 
      author={Zheng Yuan and Hongyi Yuan and Chengpeng Li and Guanting Dong and Keming Lu and Chuanqi Tan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.01825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
      journal={CoRR},
  volume={abs/2305.20050},
      year={2023}
}

@inproceedings{math,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = {NeurIPS Datasets and Benchmarks},
  year         = {2021}
}

@article{math401,
  title={How well do Large Language Models perform in Arithmetic tasks?},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang},
  journal={CoRR},
  volume={abs/2304.02015},
  year={2023}
}

@inproceedings{Wang2017DeepNS,
  title={Deep Neural Solver for Math Word Problems},
  author={Yan Wang and Xiaojiang Liu and Shuming Shi},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:910689}
}

@article{coig,
  title={Chinese open instruction generalist: A preliminary release},
  author={Zhang, Ge and Shi, Yemin and Liu, Ruibo and Yuan, Ruibin and Li, Yizhi and Dong, Siwei and Shu, Yu and Li, Zhaoqun and Wang, Zekun and Lin, Chenghua and others},
  journal={CoRR},
  volume={abs/2304.07987},
  year={2023}
}

@misc{alpaca-cot,
  author = {Qingyi Si and Tong Wang and Naibin Gu and Rui Liu and Zheng Lin },
  insititution = {Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China},
  title = {{Alpaca-CoT}: An Instruction-Tuning Platform with Unified Interface of Instruction Collection, Parameter-efficient Methods, and Large Language Models},
  year = {2023},
  url = {https://github.com/PhoebusSi/alpaca-CoT},
}

@article{wizardmath,
  title={{WizardMath}: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={CoRR},
  volume={abs/2308.09583},
  year={2023}
}

@article{azerbayev2023proofnet,
  title={{ProofNet}: Autoformalizing and formally proving undergraduate-level mathematics},
  author={Azerbayev, Zhangir and Piotrowski, Bartosz and Schoelkopf, Hailey and Ayers, Edward W and Radev, Dragomir and Avigad, Jeremy},
  journal={CoRR},
  volume={abs/2302.12433},
  year={2023}
}

@article{liu2023goat,
  title={Goat: Fine-tuned {LLaMA} Outperforms {GPT-4} on Arithmetic Tasks},
  author={Liu, Tiedong and Low, Bryan Kian Hsiang},
  journal={CoRR},
  volume={abs/2305.14201},
  year={2023}
}

@article{AlphaCode,
  author       = {Yujia Li and
                  David H. Choi and
                  Junyoung Chung and
                  Nate Kushman and
                  Julian Schrittwieser and
                  R{\'{e}}mi Leblond and
                  Tom Eccles and
                  James Keeling and
                  Felix Gimeno and
                  Agustin Dal Lago and
                  Thomas Hubert and
                  Peter Choy and
                  Cyprien de Masson d'Autume and
                  Igor Babuschkin and
                  Xinyun Chen and
                  Po{-}Sen Huang and
                  Johannes Welbl and
                  Sven Gowal and
                  Alexey Cherepanov and
                  James Molloy and
                  Daniel J. Mankowitz and
                  Esme Sutherland Robson and
                  Pushmeet Kohli and
                  Nando de Freitas and
                  Koray Kavukcuoglu and
                  Oriol Vinyals},
  title        = {Competition-Level Code Generation with {AlphaCode}},
  journal      = {CoRR},
  volume       = {abs/2203.07814},
  year         = {2022},
}

@article{LaMDA,
  author       = {Romal Thoppilan and
                  Daniel De Freitas and
                  Jamie Hall and
                  Noam Shazeer and
                  Apoorv Kulshreshtha and
                  Heng{-}Tze Cheng and
                  Alicia Jin and
                  Taylor Bos and
                  Leslie Baker and
                  Yu Du and
                  YaGuang Li and
                  Hongrae Lee and
                  Huaixiu Steven Zheng and
                  Amin Ghafouri and
                  Marcelo Menegali and
                  Yanping Huang and
                  Maxim Krikun and
                  Dmitry Lepikhin and
                  James Qin and
                  Dehao Chen and
                  Yuanzhong Xu and
                  Zhifeng Chen and
                  Adam Roberts and
                  Maarten Bosma and
                  Yanqi Zhou and
                  Chung{-}Ching Chang and
                  Igor Krivokon and
                  Will Rusch and
                  Marc Pickett and
                  Kathleen S. Meier{-}Hellstern and
                  Meredith Ringel Morris and
                  Tulsee Doshi and
                  Renelito Delos Santos and
                  Toju Duke and
                  Johnny Soraker and
                  Ben Zevenbergen and
                  Vinodkumar Prabhakaran and
                  Mark Diaz and
                  Ben Hutchinson and
                  Kristen Olson and
                  Alejandra Molina and
                  Erin Hoffman{-}John and
                  Josh Lee and
                  Lora Aroyo and
                  Ravi Rajakumar and
                  Alena Butryna and
                  Matthew Lamm and
                  Viktoriya Kuzmina and
                  Joe Fenton and
                  Aaron Cohen and
                  Rachel Bernstein and
                  Ray Kurzweil and
                  Blaise Ag{\"{u}}era y Arcas and
                  Claire Cui and
                  Marian Croak and
                  Ed H. Chi and
                  Quoc Le},
  title        = {{LaMDA}: Language Models for Dialog Applications},
  journal      = {CoRR},
  volume       = {abs/2201.08239},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.08239},
  eprinttype    = {arXiv},
  eprint       = {2201.08239},
}

@article{livecodebench,
  author       = {Naman Jain and
                  King Han and
                  Alex Gu and
                  Wen{-}Ding Li and
                  Fanjia Yan and
                  Tianjun Zhang and
                  Sida Wang and
                  Armando Solar{-}Lezama and
                  Koushik Sen and
                  Ion Stoica},
  title        = {{LiveCodeBench}: Holistic and Contamination Free Evaluation of Large
                  Language Models for Code},
  journal      = {CoRR},
  volume       = {abs/2403.07974},
  year         = {2024}
}

@article{codex,
  author       = {Mark Chen and
                  Jerry Tworek and
                  Heewoo Jun and
                  Qiming Yuan and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jared Kaplan and
                  Harrison Edwards and
                  Yuri Burda and
                  Nicholas Joseph and
                  Greg Brockman and
                  Alex Ray and
                  Raul Puri and
                  Gretchen Krueger and
                  Michael Petrov and
                  Heidy Khlaaf and
                  Girish Sastry and
                  Pamela Mishkin and
                  Brooke Chan and
                  Scott Gray and
                  Nick Ryder and
                  Mikhail Pavlov and
                  Alethea Power and
                  Lukasz Kaiser and
                  Mohammad Bavarian and
                  Clemens Winter and
                  Philippe Tillet and
                  Felipe Petroski Such and
                  Dave Cummings and
                  Matthias Plappert and
                  Fotios Chantzis and
                  Elizabeth Barnes and
                  Ariel Herbert{-}Voss and
                  William Hebgen Guss and
                  Alex Nichol and
                  Alex Paino and
                  Nikolas Tezak and
                  Jie Tang and
                  Igor Babuschkin and
                  Suchir Balaji and
                  Shantanu Jain and
                  William Saunders and
                  Christopher Hesse and
                  Andrew N. Carr and
                  Jan Leike and
                  Joshua Achiam and
                  Vedant Misra and
                  Evan Morikawa and
                  Alec Radford and
                  Matthew Knight and
                  Miles Brundage and
                  Mira Murati and
                  Katie Mayer and
                  Peter Welinder and
                  Bob McGrew and
                  Dario Amodei and
                  Sam McCandlish and
                  Ilya Sutskever and
                  Wojciech Zaremba},
  title        = {Evaluating Large Language Models Trained on Code},
  journal      = {CoRR},
  volume       = {abs/2107.03374},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03374},
  eprinttype    = {arXiv},
  eprint       = {2107.03374},
}
@misc{Azure,
  author = {Microsoft},
  insititution = {Microsoft},
  title = {Azure openai service models},
  year = {2023},
  url = {https://learn.microsoft.com/en-us/azure/
cognitive-services/openai/concepts/models},
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford {Alpaca}: An Instruction-following {LLaMA} model},
  year = {2023},
  url = {https://github.com/tatsu-lab/stanford_alpaca},
}

@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code {Alpaca}: An Instruction-following {LLaMA} model for code generation},
  year = {2023},
  url = {https://github.com/sahil280114/codealpaca},
}

@article{lora,
  title={{LoRA}: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={CoRR},
  volume={abs/2106.09685},
  year={2021}
}

@article{chatdb,
  title={ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory},
  author={Hu, Chenxu and Fu, Jie and Du, Chenzhuang and Luo, Simian and Zhao, Junbo and Zhao, Hang},
  journal={CoRR},
  volume={abs/2306.03901},
  year={2023}
}

@article{memorybank,
  title={{MemoryBank}: Enhancing Large Language Models with Long-Term Memory},
  author={Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Wang, Yanlin},
  journal={CoRR},
  volume={abs/2305.10250},
  year={2023}
}

@article{webgpt,
  title={{WebGPT}: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={CoRR},
  volume={abs/2112.09332},
  year={2021}
}

@article{webglm,
  title={{WebGLM}: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences},
  author={Liu, Xiao and Lai, Hanyu and Yu, Hao and Xu, Yifan and Zeng, Aohan and Du, Zhengxiao and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  journal={CoRR},
  volume={abs/2306.07906},
  year={2023}
}

@article{hugginggpt,
  title={{HuggingGPT}: Solving {AI} tasks with {ChatGPT} and its friends in {HuggingFace}},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={CoRR},
  volume={abs/2303.17580},
  year={2023}
}

@article{modelscope_agent,
  title={{ModelScope-Agent}: Building Your Customizable Agent System with Open-source Large Language Models},
  author={Li, Chenliang and Chen, Hehong and Yan, Ming and Shen, Weizhou and Xu, Haiyang and Wu, Zhikai and Zhang, Zhicheng and Zhou, Wenmeng and Chen, Yingda and Cheng, Chen and others},
  journal={CoRR},
  volume={abs/2309.00986},
  year={2023}
}

@article{qlora,
  title={{QLoRA}: Efficient finetuning of quantized {LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={CoRR},
  volume={abs/2305.14314},
  year={2023}
}

@article{agentverse,
  title={AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents},
  author={Chen, Weize and Su, Yusheng and Zuo, Jingwei and Yang, Cheng and Yuan, Chenfei and Qian, Chen and Chan, Chi-Min and Qin, Yujia and Lu, Yaxi and Xie, Ruobing and others},
  journal={CoRR},
  volume={abs/2308.10848},
  year={2023}
}

@article{camel,
  title={Camel: Communicative agents for ``mind'' exploration of large scale language model society},
  author={Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  journal={CoRR},
  volume={abs/2303.17760},
  year={2023}
}

@article{voyager,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={CoRR},
  volume={abs/2305.16291},
  year={2023}
}

@article{metagpt,
  title={Metagpt: Meta programming for multi-agent collaborative framework},
  author={Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and others},
  journal={CoRR},
  volume={abs/2308.00352},
  year={2023}
}

@article{santacoder,
  title={{SantaCoder}: Don't reach for the stars!},
  author={Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and others},
  journal={CoRR},
  volume={abs/2301.03988},
  year={2023}
}

@article{palm-e,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={CoRR},
  volume={abs/2303.03378},
  year={2023}
}

@article{incoder,
  title={InCoder: A Generative Model for Code Infilling and Synthesis},
  author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida I. Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Wen-tau Yih and Luke Zettlemoyer and Mike Lewis},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.05999}
}

@article{scratchpad,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Maxwell Nye and Anders Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.00114}
}

@article{self_consistency,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Huai-hsin Chi and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.11171}
}

@article{least_to_most,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Denny Zhou and Nathanael Scharli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Olivier Bousquet and Quoc Le and Ed Huai-hsin Chi},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.10625}
}

@article{codet5,
  title={{CodeT5}: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={CoRR},
  volume={abs/2109.00859},
  year={2021}
}

@article{werewolf,
  title={Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf},
  author={Xu, Yuzhuang and Wang, Shuo and Li, Peng and Luo, Fuwen and Wang, Xiaolong and Liu, Weidong and Liu, Yang},
  journal={CoRR},
  volume={abs/2309.04658},
  year={2023}
}

@article{bnb,
  title={{LLM.int8()}: 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={CoRR},
  volume={abs/2208.07339},
  year={2022}
}

@article{belle,
  title={Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases},
  author={Ji, Yunjie and Deng, Yong and Gong, Yan and Peng, Yiping and Niu, Qiang and Zhang, Lei and Ma, Baochang and Li, Xiangang},
  journal={CoRR},
  volume={abs/2303.14742},
  year={2023}
}

@misc{vicuna,
    title = {Vicuna: An Open-Source Chatbot Impressing {GPT-4} with 90\%* {ChatGPT} Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{tulu,
  title={How Far Can Camels Go? {Exploring} the State of Instruction Tuning on Open Resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi Raghavi and Wadden, David and MacMillan, Kelsey and Smith, Noah A and Beltagy, Iz and others},
  journal={CoRR},
  volume={abs/2306.04751},
  year={2023}
}

@misc{firefly,
  author = {Jianxin Yang},
  title = {Firefly},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yangjianxin1/Firefly}},
}

@article{gptq,
  title={{GPTQ}: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={CoRR},
  volume={abs/2210.17323},
  year={2022}
}

@inproceedings{pageattention,
  title={Efficient Memory Management for Large Language Model Serving with {PagedAttention}}, 
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={CoRR},
  volume={abs/2304.05302},
  year={2023}
}

@article{pro,
  title={Preference ranking optimization for human alignment},
  author={Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng},
  journal={CoRR},
  volume={abs/2306.17492},
  year={2023}
}

@misc{leetcode-solutions,
  author = {Eric Hartford},
title = {leetcode-solutions},
year = {2023},
url = {https://www.kaggle.com/datasets/erichartford/leetcode-solutions}
}

@misc{autogpt,
  author = {AutoGPT},
title = {{AutoGPT}: The heart of the open-source agent ecosystem},
year = {2023},
url = {https://github.com/Significant-Gravitas/Auto-GPT}
}

@misc{leetcode-solutions-python,
  author = {Le Vu Minh Huy},
title = {leetcode-solutions-python},
year = {2023},
url = {https://huggingface.co/datasets/mhhmm/leetcode-solutions-python}
}

@misc{evol,
author = {Nick Roshdieh},
title = {evol-teacher: Open Source {WizardCoder} Dataset},
year = {2023},
url = {https://github.com/nickrosh/evol-teacher}
}

@misc{codefuse-evol-instrution-66k,
author = {{CodeFuse}},
title = {{CodeFuse} Evol-Instruction-66k},
year = {2023},
url = {https://huggingface.co/datasets/codefuse-ai/Evol-instruction-66k}
}

@misc{codefuse-code-exercise-python-27k,
author = {{CodeFuse}},
title = {{CodeFuse} CodeExercise-Python-27k},
year = {2023},
url = {https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k}
}

@misc{TAL-SCQ5K-CN,
author={{MathGPT}},
title = {TAL-SCQ5K-CN Dataset},
year={2023},
url={https://www.mathgpt.com}
}

@inproceedings{codegen,
  author       = {Erik Nijkamp and
                  Bo Pang and
                  Hiroaki Hayashi and
                  Lifu Tu and
                  Huan Wang and
                  Yingbo Zhou and
                  Silvio Savarese and
                  Caiming Xiong},
  title        = {{CodeGen}: An Open Large Language Model for Code with Multi-Turn Program
                  Synthesis},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
}
@article{CodeGeeX,
  author       = {Qinkai Zheng and
                  Xiao Xia and
                  Xu Zou and
                  Yuxiao Dong and
                  Shan Wang and
                  Yufei Xue and
                  Zihan Wang and
                  Lei Shen and
                  Andi Wang and
                  Yang Li and
                  Teng Su and
                  Zhilin Yang and
                  Jie Tang},
  title        = {{CodeGeeX}: {A} Pre-Trained Model for Code Generation with Multilingual
                  Evaluations on HumanEval-X},
  journal      = {CoRR},
  volume       = {abs/2303.17568},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.17568},
  doi          = {10.48550/arXiv.2303.17568},
  eprinttype    = {arXiv},
  eprint       = {2303.17568},
}

@article{CodeT5+,
  author       = {Yue Wang and
                  Hung Le and
                  Akhilesh Deepak Gotmare and
                  Nghi D. Q. Bui and
                  Junnan Li and
                  Steven C. H. Hoi},
  title        = {{CodeT5+}: Open Code Large Language Models for Code Understanding and
                  Generation},
  journal      = {CoRR},
  volume       = {abs/2305.07922},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.07922},
  doi          = {10.48550/arXiv.2305.07922},
  eprinttype    = {arXiv},
  eprint       = {2305.07922},
}

@inproceedings{self_instruct,
  author       = {Yizhong Wang and
                  Yeganeh Kordi and
                  Swaroop Mishra and
                  Alisa Liu and
                  Noah A. Smith and
                  Daniel Khashabi and
                  Hannaneh Hajishirzi},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {{Self-Instruct}: Aligning Language Models with Self-Generated Instructions},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {13484--13508},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.acl-long.754},
  doi          = {10.18653/v1/2023.acl-long.754},
  timestamp    = {Thu, 10 Aug 2023 12:35:44 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/WangKMLSKH23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Wei2022EmergentAO,
  title={Emergent Abilities of Large Language Models},
  author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed Huai-hsin Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  journal={Trans. Mach. Learn. Res.},
  year={2022},
  volume={2022},
  url={https://api.semanticscholar.org/CorpusID:249674500}
}
@article{octopack,
  author       = {Niklas Muennighoff and
                  Qian Liu and
                  Armel Zebaze and
                  Qinkai Zheng and
                  Binyuan Hui and
                  Terry Yue Zhuo and
                  Swayam Singh and
                  Xiangru Tang and
                  Leandro von Werra and
                  Shayne Longpre},
  title        = {{OctoPack}: Instruction Tuning Code Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2308.07124},
  year         = {2023},
}

@misc{yuan2023rrhf,
      title={{RRHF}: Rank Responses to Align Language Models with Human Feedback without tears}, 
      author={Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Fei Huang},
      year={2023},
      eprint={2304.05302},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ntk,
      title={{NTK}-Aware Scaled {RoPE} allows {LLaMA} models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.}, 
      author={bloc97},
      year={2023},
      url={https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}
}

@inproceedings{logn_attn,
  title={Overcoming a Theoretical Limitation of Self-Attention},
  author={Chiang, David and Cholak, Peter},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7654--7664},
  year={2022}
}

@article{window_attn,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={CoRR},
  volume={abs/2004.05150},
  year={2020}
}

@misc{qkv_bias,
  title={The magical effect of the {Bias} term: {RoPE} + {Bias} = better length extrapolation},
  author={Jianlin Su},
  year={2023},
  url={https://spaces.ac.cn/archives/9577}
}




@article{wizardcoder,
  title={{WizardCoder}: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={CoRR},
  volume={abs/2306.08568},
  year={2023}
}


@article{PMP,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={CoRR},
  volume={abs/2204.05862},
  year={2022}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={CoRR},
  volume={abs/2112.00861},
  year={2021}
}

@article{bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={CoRR},
  volume={abs/1810.04805},
  year={2018}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={CoRR},
  volume={abs/1707.06347},
  year={2017}
}

@article{learn-summary,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{yao2022react,
  title={{ReAct}: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={CoRR},
  volume={abs/2210.03629},
  year={2022}
}

@article{yarn,
  author       = {Bowen Peng and
                  Jeffrey Quesnelle and
                  Honglu Fan and
                  Enrico Shippole},
  title        = {{YaRN}: Efficient Context Window Extension of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.00071},
  year         = {2023}
}

@inproceedings{upcycle,
  author       = {Aran Komatsuzaki and
                  Joan Puigcerver and
                  James Lee{-}Thorp and
                  Carlos Riquelme Ruiz and
                  Basil Mustafa and
                  Joshua Ainslie and
                  Yi Tay and
                  Mostafa Dehghani and
                  Neil Houlsby},
  title        = {Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2023}
}

@article{deepseekmoe,
  author       = {Damai Dai and
                  Chengqi Deng and
                  Chenggang Zhao and
                  R. X. Xu and
                  Huazuo Gao and
                  Deli Chen and
                  Jiashi Li and
                  Wangding Zeng and
                  Xingkai Yu and
                  Y. Wu and
                  Zhenda Xie and
                  Y. K. Li and
                  Panpan Huang and
                  Fuli Luo and
                  Chong Ruan and
                  Zhifang Sui and
                  Wenfeng Liang},
  title        = {{DeepSeekMoE}: Towards Ultimate Expert Specialization in Mixture-of-Experts
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2401.06066},
  year         = {2024}
}

@inproceedings{deepspeedmoe,
  author       = {Samyam Rajbhandari and
                  Conglong Li and
                  Zhewei Yao and
                  Minjia Zhang and
                  Reza Yazdani Aminabadi and
                  Ammar Ahmad Awan and
                  Jeff Rasley and
                  Yuxiong He},
  title        = {{DeepSpeed-MoE}: Advancing Mixture-of-Experts Inference and Training
                  to Power Next-Generation {AI} Scale},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {18332--18346},
  publisher    = {{PMLR}},
  year         = {2022}
}

@article{qwen,
author       = {Jinze Bai and
                  Shuai Bai and
                  Yunfei Chu and
                  Zeyu Cui and
                  Kai Dang and
                  Xiaodong Deng and
                  Yang Fan and
                  Wenbin Ge and
                  Yu Han and
                  Fei Huang and
                  Binyuan Hui and
                  Luo Ji and
                  Mei Li and
                  Junyang Lin and
                  Runji Lin and
                  Dayiheng Liu and
                  Gao Liu and
                  Chengqiang Lu and
                  Keming Lu and
                  Jianxin Ma and
                  Rui Men and
                  Xingzhang Ren and
                  Xuancheng Ren and
                  Chuanqi Tan and
                  Sinan Tan and
                  Jianhong Tu and
                  Peng Wang and
                  Shijie Wang and
                  Wei Wang and
                  Shengguang Wu and
                  Benfeng Xu and
                  Jin Xu and
                  An Yang and
                  Hao Yang and
                  Jian Yang and
                  Shusheng Yang and
                  Yang Yao and
                  Bowen Yu and
                  Hongyi Yuan and
                  Zheng Yuan and
                  Jianwei Zhang and
                  Xingxuan Zhang and
                  Yichang Zhang and
                  Zhenru Zhang and
                  Chang Zhou and
                  Jingren Zhou and
                  Xiaohuan Zhou and
                  Tianhang Zhu},
  title        = {Qwen Technical Report},
  journal      = {CoRR},
  volume       = {abs/2309.16609},
  year         = {2023}
}

@inproceedings{gqa,
  author       = {Joshua Ainslie and
                  James Lee{-}Thorp and
                  Michiel de Jong and
                  Yury Zemlyanskiy and
                  Federico Lebr{\'{o}}n and
                  Sumit Sanghai},
  title        = {{GQA}: Training Generalized Multi-Query {Transformer} Models from Multi-Head
                  Checkpoints},
  booktitle    = {{EMNLP}},
  pages        = {4895--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@article{cot,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@misc{taylor2022galactica,
      title={Galactica: A Large Language Model for Science}, 
      author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
      year={2022},
      eprint={2211.09085},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lewkowycz2022solving,
      title={Solving Quantitative Reasoning Problems with Language Models}, 
      author={Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and Henryk Michalewski and Vinay Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
      year={2022},
      eprint={2206.14858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yue2023mammoth,
  title={{MAmmoTH}: Building Math Generalist Models through Hybrid Instruction Tuning},
  author={Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal={CoRR},
  volume={abs/2309.05653},
  year={2023}
}


@inproceedings{code_translation_compiler,
  author       = {Marc Szafraniec and
                  Baptiste Rozi{\`{e}}re and
                  Hugh Leather and
                  Patrick Labatut and
                  Fran{\c{c}}ois Charton and
                  Gabriel Synnaeve},
  title        = {Code Translation with Compiler Representations},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=XomEU3eNeSQ},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/SzafraniecRLLCS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{repocoder,
  author       = {Fengji Zhang and
                  Bei Chen and
                  Yue Zhang and
                  Jin Liu and
                  Daoguang Zan and
                  Yi Mao and
                  Jian{-}Guang Lou and
                  Weizhu Chen},
  title        = {{RepoCoder}: Repository-Level Code Completion Through Iterative Retrieval
                  and Generation},
  journal      = {CoRR},
  volume       = {abs/2303.12570},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.12570},
  doi          = {10.48550/arXiv.2303.12570},
  eprinttype    = {arXiv},
  eprint       = {2303.12570},
  timestamp    = {Thu, 13 Apr 2023 17:40:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-12570.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{code_refinement,
  author       = {Yue Liu and
                  Thanh Le{-}Cong and
                  Ratnadira Widyasari and
                  Chakkrit Tantithamthavorn and
                  Li Li and
                  Xuan{-}Bach Dinh Le and
                  David Lo},
  title        = {Refining {ChatGPT}-Generated Code: Characterizing and Mitigating Code
                  Quality Issues},
  journal      = {CoRR},
  volume       = {abs/2307.12596},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.12596},
  doi          = {10.48550/arXiv.2307.12596},
  eprinttype    = {arXiv},
  eprint       = {2307.12596},
  timestamp    = {Tue, 01 Aug 2023 14:49:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-12596.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{code_qa,
  author       = {Chenxiao Liu and
                  Xiaojun Wan},
  editor       = {Marie{-}Francine Moens and
                  Xuanjing Huang and
                  Lucia Specia and
                  Scott Wen{-}tau Yih},
  title        = {{CodeQA}: {A} Question Answering Dataset for Source Code Comprehension},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
                  2021},
  pages        = {2618--2632},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.findings-emnlp.223},
  doi          = {10.18653/v1/2021.findings-emnlp.223},
  timestamp    = {Thu, 20 Jan 2022 10:02:06 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/Liu021.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{logn_su,
  title={Improving Transformer: Length Extrapolation Ability and Position Robustness},
  author={Jianlin Su},
  year={2023},
  url={https://spaces.ac.cn/archives/9444}
}


@article{code_search_net,
  author       = {Hamel Husain and
                  Ho{-}Hsiang Wu and
                  Tiferet Gazit and
                  Miltiadis Allamanis and
                  Marc Brockschmidt},
  title        = {CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  journal      = {CoRR},
  volume       = {abs/1909.09436},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.09436},
  eprinttype    = {arXiv},
  eprint       = {1909.09436},
  timestamp    = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-09436.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{agieval,
  author       = {Wanjun Zhong and
                  Ruixiang Cui and
                  Yiduo Guo and
                  Yaobo Liang and
                  Shuai Lu and
                  Yanlin Wang and
                  Amin Saied and
                  Weizhu Chen and
                  Nan Duan},
  title        = {{AGIEval}: {A} Human-Centric Benchmark for Evaluating Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2304.06364},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.06364},
  doi          = {10.48550/arXiv.2304.06364},
  eprinttype    = {arXiv},
  eprint       = {2304.06364},
  timestamp    = {Wed, 19 Apr 2023 12:42:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-06364.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{gaokao-bench,
  author       = {Xiaotian Zhang and
                  Chunyang Li and
                  Yi Zong and
                  Zhengyu Ying and
                  Liang He and
                  Xipeng Qiu},
  title        = {Evaluating the Performance of Large Language Models on {GAOKAO} Benchmark},
  journal      = {CoRR},
  volume       = {abs/2305.12474},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.12474},
  doi          = {10.48550/arXiv.2305.12474},
  eprinttype    = {arXiv},
  eprint       = {2305.12474},
  timestamp    = {Fri, 26 May 2023 11:29:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-12474.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{arc,
  author       = {Peter Clark and
                  Isaac Cowhey and
                  Oren Etzioni and
                  Tushar Khot and
                  Ashish Sabharwal and
                  Carissa Schoenick and
                  Oyvind Tafjord},
  title        = {Think you have Solved Question Answering? {Try} {ARC}, the {AI2} Reasoning
                  Challenge},
  journal      = {CoRR},
  volume       = {abs/1803.05457},
  year         = {2018}
}



@inproceedings{boolq,
  author       = {Christopher Clark and
                  Kenton Lee and
                  Ming{-}Wei Chang and
                  Tom Kwiatkowski and
                  Michael Collins and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {2924--2936},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1300},
  doi          = {10.18653/v1/n19-1300},
  timestamp    = {Tue, 16 Aug 2022 23:04:27 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/ClarkLCK0T19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{commonsenseqa,
  author       = {Alon Talmor and
                  Jonathan Herzig and
                  Nicholas Lourie and
                  Jonathan Berant},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{CommonsenseQA}: {A} Question Answering Challenge Targeting Commonsense
                  Knowledge},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4149--4158},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1421},
  doi          = {10.18653/v1/n19-1421},
  timestamp    = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/TalmorHLB19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{naturalquestions,
  author       = {Tom Kwiatkowski and
                  Jennimaria Palomaki and
                  Olivia Redfield and
                  Michael Collins and
                  Ankur P. Parikh and
                  Chris Alberti and
                  Danielle Epstein and
                  Illia Polosukhin and
                  Jacob Devlin and
                  Kenton Lee and
                  Kristina Toutanova and
                  Llion Jones and
                  Matthew Kelcey and
                  Ming{-}Wei Chang and
                  Andrew M. Dai and
                  Jakob Uszkoreit and
                  Quoc Le and
                  Slav Petrov},
  title        = {Natural Questions: a Benchmark for Question Answering Research},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {7},
  pages        = {452--466},
  year         = {2019},
  url          = {https://doi.org/10.1162/tacl\_a\_00276},
  doi          = {10.1162/tacl\_a\_00276},
  timestamp    = {Tue, 16 Aug 2022 23:05:11 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/KwiatkowskiPRCP19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{pmlr-v162-ethayarajh22a,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher = {PMLR},
}



@inproceedings{lambada,
  author       = {Denis Paperno and
                  Germ{\'{a}}n Kruszewski and
                  Angeliki Lazaridou and
                  Quan Ngoc Pham and
                  Raffaella Bernardi and
                  Sandro Pezzelle and
                  Marco Baroni and
                  Gemma Boleda and
                  Raquel Fern{\'{a}}ndez},
  title        = {The {LAMBADA} dataset: Word prediction requiring a broad discourse
                  context},
  booktitle    = {Proceedings of the 54th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume
                  1: Long Papers},
  publisher    = {The Association for Computer Linguistics},
  year         = {2016},
  url          = {https://doi.org/10.18653/v1/p16-1144},
  doi          = {10.18653/v1/p16-1144},
  timestamp    = {Fri, 06 Aug 2021 00:41:02 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/PapernoKLPBPBBF16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{ocnli,
  author       = {Hai Hu and
                  Kyle Richardson and
                  Liang Xu and
                  Lu Li and
                  Sandra K{\"{u}}bler and
                  Lawrence S. Moss},
  editor       = {Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {{OCNLI:} Original Chinese Natural Language Inference},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2020, Online Event, 16-20 November 2020},
  series       = {Findings of {ACL}},
  volume       = {{EMNLP} 2020},
  pages        = {3512--3526},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.findings-emnlp.314},
  doi          = {10.18653/v1/2020.findings-emnlp.314},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/HuRXLKM20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{hellaswag,
  author       = {Rowan Zellers and
                  Ari Holtzman and
                  Yonatan Bisk and
                  Ali Farhadi and
                  Yejin Choi},
  title        = {{HellaSwag}: Can a Machine Really Finish Your Sentence?},
  booktitle    = {{ACL} {(1)}},
  pages        = {4791--4800},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}



@inproceedings{piqa,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {7432--7439},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/aaai.v34i05.6239},
  timestamp    = {Mon, 04 Sep 2023 16:50:23 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{arena-hard,
      title={From Crowdsourced Data to High-Quality Benchmarks: {Arena-Hard} and {BenchBuilder} Pipeline}, 
      author={Tianle Li and Wei-Lin Chiang and Evan Frick and Lisa Dunlap and Tianhao Wu and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
      year={2024},
      journal={CoRR},
      volume={abs/2406.11939}
}

@article{mixeval,
  title={{MixEval}: Deriving Wisdom of the Crowd from {LLM} Benchmark Mixtures},
  author={Ni, Jinjie and Xue, Fuzhao and Yue, Xiang and Deng, Yuntian and Shah, Mahir and Jain, Kabir and Neubig, Graham and You, Yang},
  journal={CoRR}, 
  volume={abs/2406.06565},
  year={2024}
}

@article{alignbench,
  author       = {Xiao Liu and
                  Xuanyu Lei and
                  Shengyuan Wang and
                  Yue Huang and
                  Zhuoer Feng and
                  Bosi Wen and
                  Jiale Cheng and
                  Pei Ke and
                  Yifan Xu and
                  Weng Lam Tam and
                  Xiaohan Zhang and
                  Lichao Sun and
                  Hongning Wang and
                  Jing Zhang and
                  Minlie Huang and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {{AlignBench}: Benchmarking {Chinese} Alignment of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.18743},
  year         = {2023}
}


@article{siqa,
  author       = {Maarten Sap and
                  Hannah Rashkin and
                  Derek Chen and
                  Ronan Le Bras and
                  Yejin Choi},
  title        = {{SocialIQA}: Commonsense Reasoning about Social Interactions},
  journal      = {CoRR},
  volume       = {abs/1904.09728},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.09728},
  eprinttype    = {arXiv},
  eprint       = {1904.09728},
  timestamp    = {Sat, 29 Apr 2023 10:09:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09728.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@misc{abel,
  author = {Chern, Ethan and Zou, Haoyang and Li, Xuefeng and Hu, Jiewen and Feng, Kehua and Li, Junlong and Liu, Pengfei},
  title = {Generative AI for Math: Abel},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/GAIR-NLP/abel}},
}

@misc{yu2023metamath,
      title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}, 
      author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
      year={2023},
      eprint={2309.12284},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{chern2023factool,
  title={FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},
  author={Chern, I and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei and others},
  journal={CoRR},
  volume={abs/2307.13528},
  year={2023}
}

@article{position_interpolation,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={CoRR},
  volume={abs/2306.15595},
  year={2023}
}

@inproceedings{evalplus,
  author       = {Jiawei Liu and
                  Chunqiu Steven Xia and
                  Yuyao Wang and
                  Lingming Zhang},
  title        = {Is Your Code Generated by {ChatGPT} Really Correct? {Rigorous} Evaluation
                  of Large Language Models for Code Generation},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{chunkllama,
  author       = {Chenxin An and
                  Fei Huang and
                  Jun Zhang and
                  Shansan Gong and
                  Xipeng Qiu and
                  Chang Zhou and
                  Lingpeng Kong},
  title        = {Training-Free Long-Context Scaling of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.17463},
  year         = {2024}
}


@article{ropeabf,
  author       = {Wenhan Xiong and
                  Jingyu Liu and
                  Igor Molybog and
                  Hejia Zhang and
                  Prajjwal Bhargava and
                  Rui Hou and
                  Louis Martin and
                  Rashi Rungta and
                  Karthik Abinav Sankararaman and
                  Barlas Oguz and
                  Madian Khabsa and
                  Han Fang and
                  Yashar Mehdad and
                  Sharan Narang and
                  Kshitiz Malik and
                  Angela Fan and
                  Shruti Bhosale and
                  Sergey Edunov and
                  Mike Lewis and
                  Sinong Wang and
                  Hao Ma},
  title        = {Effective Long-Context Scaling of Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2309.16039},
  year         = {2023}
}

@inproceedings{koto-etal-2023-indommlu,
  author       = {Fajri Koto and
                  Nurul Aisyah and
                  Haonan Li and
                  Timothy Baldwin},
  title        = {Large Language Models Only Pass Primary School Exams in {Indonesia}:
                  {A} Comprehensive Test on {IndoMMLU}},
  booktitle    = {{EMNLP}},
  pages        = {12359--12374},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@article{revaut2024how,
  author       = {Mathieu Ravaut and
                  Bosheng Ding and
                  Fangkai Jiao and
                  Hailin Chen and
                  Xingxuan Li and
                  Ruochen Zhao and
                  Chengwei Qin and
                  Caiming Xiong and
                  Shafiq Joty},
  title        = {How Much are {LLMs} Contaminated? {A} Comprehensive Survey and the LLMSanitize
                  Library},
  journal      = {CoRR},
  volume       = {abs/2404.00699},
  year         = {2024}
}

@inproceedings{sainz2023nlp,
  author       = {Oscar Sainz and
                  Jon Ander Campos and
                  Iker Garc{\'{\i}}a{-}Ferrero and
                  Julen Etxaniz and
                  Oier Lopez de Lacalle and
                  Eneko Agirre},
  title        = {{NLP} Evaluation in trouble: On the Need to Measure {LLM} Data Contamination
                  for each Benchmark},
  booktitle    = {{EMNLP} (Findings)},
  pages        = {10776--10787},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@inproceedings{golchin2024time,
  author       = {Shahriar Golchin and
                  Mihai Surdeanu},
  title        = {Time Travel in LLMs: Tracing Data Contamination in Large Language
                  Models},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@article{rummlu-mera,
  author       = {Alena Fenogenova and
                  Artem Chervyakov and
                  Nikita Martynov and
                  Anastasia Kozlova and
                  Maria Tikhonova and
                  Albina Akhmetgareeva and
                  Anton A. Emelyanov and
                  Denis Shevelev and
                  Pavel Lebedev and
                  Leonid Sinev and
                  Ulyana Isaeva and
                  Katerina Kolomeytseva and
                  Daniil Moskovskiy and
                  Elizaveta Goncharova and
                  Nikita Savushkin and
                  Polina Mikhailova and
                  Denis Dimitrov and
                  Alexander Panchenko and
                  Sergey Markov},
  title        = {{MERA:} {A} Comprehensive {LLM} Evaluation in Russian},
  journal      = {CoRR},
  volume       = {abs/2401.04531},
  year         = {2024}
}

@article{belebele,
  author       = {Lucas Bandarkar and
                  Davis Liang and
                  Benjamin Muller and
                  Mikel Artetxe and
                  Satya Narayan Shukla and
                  Donald Husa and
                  Naman Goyal and
                  Abhinandan Krishnan and
                  Luke Zettlemoyer and
                  Madian Khabsa},
  title        = {The {Belebele} Benchmark: A Parallel Reading Comprehension Dataset in
                  122 Language Variants},
  journal      = {CoRR},
  volume       = {abs/2308.16884},
  year         = {2023}
}

@inproceedings{xcopa,
  author       = {Edoardo Maria Ponti and
                  Goran Glavas and
                  Olga Majewska and
                  Qianchu Liu and
                  Ivan Vulic and
                  Anna Korhonen},
  title        = {{XCOPA}: {A} Multilingual Dataset for Causal Commonsense Reasoning},
  booktitle    = {{EMNLP} {(1)}},
  pages        = {2362--2376},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@inproceedings{xstory_cloze,
      author       = {Xi Victoria Lin and
                  Todor Mihaylov and
                  Mikel Artetxe and
                  Tianlu Wang and
                  Shuohui Chen and
                  Daniel Simig and
                  Myle Ott and
                  Naman Goyal and
                  Shruti Bhosale and
                  Jingfei Du and
                  Ramakanth Pasunuru and
                  Sam Shleifer and
                  Punit Singh Koura and
                  Vishrav Chaudhary and
                  Brian O'Horo and
                  Jeff Wang and
                  Luke Zettlemoyer and
                  Zornitsa Kozareva and
                  Mona T. Diab and
                  Veselin Stoyanov and
                  Xian Li},
  title        = {Few-shot Learning with Multilingual Generative Language Models},
  booktitle    = {{EMNLP}},
  pages        = {9019--9052},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}

@inproceedings{paws-x,
      author       = {Yinfei Yang and
                  Yuan Zhang and
                  Chris Tar and
                  Jason Baldridge},
  title        = {{PAWS-X:} {A} Cross-lingual Adversarial Dataset for Paraphrase Identification},
  booktitle    = {{EMNLP/IJCNLP} {(1)}},
  pages        = {3685--3690},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}

@misc{msift,
author = {Chen, Zhihong and Yan, Shuo and Liang, Juhao and Jiang, Feng and Wu, Xiangbo and Yu, Fei and Chen, Guiming Hardy and Chen, Junying and Zhang, Hongbo and Li Jianquan and Wan Xiang and Wang, Benyou},
title = {{MultilingualSIFT}: Multilingual Supervised Instruction Fine-tuning},
url = {https://github.com/FreedomIntelligence/MultilingualSIFT},
year = {2023}
}

@inproceedings{mgsm,
  author       = {Freda Shi and
                  Mirac Suzgun and
                  Markus Freitag and
                  Xuezhi Wang and
                  Suraj Srivats and
                  Soroush Vosoughi and
                  Hyung Won Chung and
                  Yi Tay and
                  Sebastian Ruder and
                  Denny Zhou and
                  Dipanjan Das and
                  Jason Wei},
  title        = {Language models are multilingual chain-of-thought reasoners},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=fR3wGCk-IXp},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ShiSF0SVCTRZ0W23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{flores,
  author       = {Naman Goyal and
                  Cynthia Gao and
                  Vishrav Chaudhary and
                  Peng{-}Jen Chen and
                  Guillaume Wenzek and
                  Da Ju and
                  Sanjana Krishnan and
                  Marc'Aurelio Ranzato and
                  Francisco Guzm{\'{a}}n and
                  Angela Fan},
  title        = {The {Flores-101} Evaluation Benchmark for Low-Resource and Multilingual
                  Machine Translation},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {10},
  pages        = {522--538},
  year         = {2022}
}

@techreport{claude3,
  title={The {Claude} 3 model family: {Opus}, {Sonnet}, {Haiku}},
  author={Anthropic},
  institution={{Anthropic, AI}},
  url={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf},
  year={2024}
}


@article{arena,
  author       = {Wei{-}Lin Chiang and
                  Lianmin Zheng and
                  Ying Sheng and
                  Anastasios Nikolas Angelopoulos and
                  Tianle Li and
                  Dacheng Li and
                  Hao Zhang and
                  Banghua Zhu and
                  Michael I. Jordan and
                  Joseph E. Gonzalez and
                  Ion Stoica},
  title        = {Chatbot Arena: An Open Platform for Evaluating {LLMs} by Human Preference},
  journal      = {CoRR},
  volume       = {abs/2403.04132},
  year         = {2024}
}

@article{llama3,
  author       = {Abhimanyu Dubey and
                  Abhinav Jauhri and
                  Abhinav Pandey and
                  Abhishek Kadian and
                  Ahmad Al{-}Dahle and
                  Aiesha Letman and
                  Akhil Mathur and
                  Alan Schelten and
                  Amy Yang and
                  Angela Fan and
                  Anirudh Goyal and
                  Anthony Hartshorn and
                  Aobo Yang and
                  Archi Mitra and
                  Archie Sravankumar and
                  Artem Korenev and
                  Arthur Hinsvark and
                  Arun Rao and
                  Aston Zhang and
                  Aur{\'{e}}lien Rodriguez and
                  Austen Gregerson and
                  Ava Spataru and
                  Baptiste Rozi{\`{e}}re and
                  Bethany Biron and
                  Binh Tang and
                  Bobbie Chern and
                  Charlotte Caucheteux and
                  Chaya Nayak and
                  Chloe Bi and
                  Chris Marra and
                  Chris McConnell and
                  Christian Keller and
                  Christophe Touret and
                  Chunyang Wu and
                  Corinne Wong and
                  Cristian Canton Ferrer and
                  Cyrus Nikolaidis and
                  Damien Allonsius and
                  Daniel Song and
                  Danielle Pintz and
                  Danny Livshits and
                  David Esiobu and
                  Dhruv Choudhary and
                  Dhruv Mahajan and
                  Diego Garcia{-}Olano and
                  Diego Perino and
                  Dieuwke Hupkes and
                  Egor Lakomkin and
                  Ehab AlBadawy and
                  Elina Lobanova and
                  Emily Dinan and
                  Eric Michael Smith and
                  Filip Radenovic and
                  Frank Zhang and
                  Gabriel Synnaeve and
                  Gabrielle Lee and
                  Georgia Lewis Anderson and
                  Graeme Nail and
                  Gr{\'{e}}goire Mialon and
                  Guan Pang and
                  Guillem Cucurell and
                  Hailey Nguyen and
                  Hannah Korevaar and
                  Hu Xu and
                  Hugo Touvron and
                  Iliyan Zarov and
                  Imanol Arrieta Ibarra and
                  Isabel M. Kloumann and
                  Ishan Misra and
                  Ivan Evtimov and
                  Jade Copet and
                  Jaewon Lee and
                  Jan Geffert and
                  Jana Vranes and
                  Jason Park and
                  Jay Mahadeokar and
                  Jeet Shah and
                  Jelmer van der Linde and
                  Jennifer Billock and
                  Jenny Hong and
                  Jenya Lee and
                  Jeremy Fu and
                  Jianfeng Chi and
                  Jianyu Huang and
                  Jiawen Liu and
                  Jie Wang and
                  Jiecao Yu and
                  Joanna Bitton and
                  Joe Spisak and
                  Jongsoo Park and
                  Joseph Rocca and
                  Joshua Johnstun and
                  Joshua Saxe and
                  Junteng Jia and
                  Kalyan Vasuden Alwala and
                  Kartikeya Upasani and
                  Kate Plawiak and
                  Ke Li and
                  Kenneth Heafield and
                  Kevin Stone and
                  et al.},
  title        = {The {Llama} 3 Herd of Models},
  journal      = {CoRR},
  volume       = {abs/2407.21783},
  year         = {2024}
}

@article{mistral,
  author       = {Albert Q. Jiang and
                  Alexandre Sablayrolles and
                  Arthur Mensch and
                  Chris Bamford and
                  Devendra Singh Chaplot and
                  Diego de Las Casas and
                  Florian Bressand and
                  Gianna Lengyel and
                  Guillaume Lample and
                  Lucile Saulnier and
                  L{\'{e}}lio Renard Lavaud and
                  Marie{-}Anne Lachaux and
                  Pierre Stock and
                  Teven Le Scao and
                  Thibaut Lavril and
                  Thomas Wang and
                  Timoth{\'{e}}e Lacroix and
                  William El Sayed},
  title        = {Mistral {7B}},
  journal      = {CoRR},
  volume       = {abs/2310.06825},
  year         = {2023}
}

@article{mixtral,
  author       = {Albert Q. Jiang and
                  Alexandre Sablayrolles and
                  Antoine Roux and
                  Arthur Mensch and
                  Blanche Savary and
                  Chris Bamford and
                  Devendra Singh Chaplot and
                  Diego de Las Casas and
                  Emma Bou Hanna and
                  Florian Bressand and
                  Gianna Lengyel and
                  Guillaume Bour and
                  Guillaume Lample and
                  L{\'{e}}lio Renard Lavaud and
                  Lucile Saulnier and
                  Marie{-}Anne Lachaux and
                  Pierre Stock and
                  Sandeep Subramanian and
                  Sophia Yang and
                  Szymon Antoniak and
                  Teven Le Scao and
                  Th{\'{e}}ophile Gervet and
                  Thibaut Lavril and
                  Thomas Wang and
                  Timoth{\'{e}}e Lacroix and
                  William El Sayed},
  title        = {Mixtral of Experts},
  journal      = {CoRR},
  volume       = {abs/2401.04088},
  year         = {2024}
}

@techreport{gemini,
  title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author = {{Gemini Team}},
  institution = {Google},
  url = {https://storage.googleapis.com/deepmind-media/gemini/gemini\_v1\_5\_report.pdf},
  year = {2024}
}

@article{gemma,
  author       = {Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
  title        = {Gemma: Open Models Based on {Gemini} Research and Technology},
  journal      = {CoRR},
  volume       = {abs/2403.08295},
  year         = {2024}
}

@article{deepseek,
author       = {Aixin Liu and
                  Bei Feng and
                  Bin Wang and
                  Bingxuan Wang and
                  Bo Liu and
                  Chenggang Zhao and
                  Chengqi Deng and
                  Chong Ruan and
                  Damai Dai and
                  Daya Guo and
                  Dejian Yang and
                  Deli Chen and
                  Dongjie Ji and
                  Erhang Li and
                  Fangyun Lin and
                  Fuli Luo and
                  Guangbo Hao and
                  Guanting Chen and
                  Guowei Li and
                  Hao Zhang and
                  Hanwei Xu and
                  Hao Yang and
                  Haowei Zhang and
                  Honghui Ding and
                  Huajian Xin and
                  Huazuo Gao and
                  Hui Li and
                  Hui Qu and
                  J. L. Cai and
                  Jian Liang and
                  Jianzhong Guo and
                  Jiaqi Ni and
                  Jiashi Li and
                  Jin Chen and
                  Jingyang Yuan and
                  Junjie Qiu and
                  Junxiao Song and
                  Kai Dong and
                  Kaige Gao and
                  Kang Guan and
                  Lean Wang and
                  Lecong Zhang and
                  Lei Xu and
                  Leyi Xia and
                  Liang Zhao and
                  Liyue Zhang and
                  Meng Li and
                  Miaojun Wang and
                  Mingchuan Zhang and
                  Minghua Zhang and
                  Minghui Tang and
                  Mingming Li and
                  Ning Tian and
                  Panpan Huang and
                  Peiyi Wang and
                  Peng Zhang and
                  Qihao Zhu and
                  Qinyu Chen and
                  Qiushi Du and
                  R. J. Chen and
                  R. L. Jin and
                  Ruiqi Ge and
                  Ruizhe Pan and
                  Runxin Xu and
                  Ruyi Chen and
                  S. S. Li and
                  Shanghao Lu and
                  Shangyan Zhou and
                  Shanhuang Chen and
                  Shaoqing Wu and
                  Shengfeng Ye and
                  Shirong Ma and
                  Shiyu Wang and
                  Shuang Zhou and
                  Shuiping Yu and
                  Shunfeng Zhou and
                  Size Zheng and
                  Tao Wang and
                  Tian Pei and
                  Tian Yuan and
                  Tianyu Sun and
                  W. L. Xiao and
                  Wangding Zeng and
                  Wei An and
                  Wen Liu and
                  Wenfeng Liang and
                  Wenjun Gao and
                  Wentao Zhang and
                  X. Q. Li and
                  Xiangyue Jin and
                  Xianzu Wang and
                  Xiao Bi and
                  Xiaodong Liu and
                  Xiaohan Wang and
                  Xiaojin Shen and
                  Xiaokang Chen and
                  Xiaosha Chen and
                  Xiaotao Nie and
                  Xiaowen Sun},
  title        = {{DeepSeek-V2}: A Strong, Economical, and Efficient Mixture-of-Experts
                  Language Model},
  journal      = {CoRR},
  volume       = {abs/2405.04434},
  year         = {2024}
}

@article{yi,
author       = {Alex Young and
                  Bei Chen and
                  Chao Li and
                  Chengen Huang and
                  Ge Zhang and
                  Guanwei Zhang and
                  Heng Li and
                  Jiangcheng Zhu and
                  Jianqun Chen and
                  Jing Chang and
                  Kaidong Yu and
                  Peng Liu and
                  Qiang Liu and
                  Shawn Yue and
                  Senbin Yang and
                  Shiming Yang and
                  Tao Yu and
                  Wen Xie and
                  Wenhao Huang and
                  Xiaohui Hu and
                  Xiaoyi Ren and
                  Xinyao Niu and
                  Pengcheng Nie and
                  Yuchi Xu and
                  Yudong Liu and
                  Yue Wang and
                  Yuxuan Cai and
                  Zhenyu Gu and
                  Zhiyuan Liu and
                  Zonghong Dai},
  title        = {Yi: Open Foundation Models by {01.AI}},
  journal      = {CoRR},
  volume       = {abs/2403.04652},
  year         = {2024}
}

@misc{dbrx,
  title = {Introducing {DBRX}: A New State-of-the-Art Open {LLM}},
  author = {{Mosaic Research Team}},
  url = {https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
  year = {2024}
}

@article{qwenaudio,
  author       = {Yunfei Chu and
                  Jin Xu and
                  Xiaohuan Zhou and
                  Qian Yang and
                  Shiliang Zhang and
                  Zhijie Yan and
                  Chang Zhou and
                  Jingren Zhou},
  title        = {{Qwen-Audio}: Advancing Universal Audio Understanding via Unified Large-Scale
                  Audio-Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.07919},
  year         = {2023}
}

@inproceedings{mtbench,
  author       = {Lianmin Zheng and
                  Wei{-}Lin Chiang and
                  Ying Sheng and
                  Siyuan Zhuang and
                  Zhanghao Wu and
                  Yonghao Zhuang and
                  Zi Lin and
                  Zhuohan Li and
                  Dacheng Li and
                  Eric P. Xing and
                  Hao Zhang and
                  Joseph E. Gonzalez and
                  Ion Stoica},
  title        = {Judging {LLM}-as-a-Judge with {MT-Bench} and {Chatbot Arena}},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{humaneval,
  author       = {Mark Chen and
                  Jerry Tworek and
                  Heewoo Jun and
                  Qiming Yuan and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jared Kaplan and
                  Harrison Edwards and
                  Yuri Burda and
                  Nicholas Joseph and
                  Greg Brockman and
                  Alex Ray and
                  Raul Puri and
                  Gretchen Krueger and
                  Michael Petrov and
                  Heidy Khlaaf and
                  Girish Sastry and
                  Pamela Mishkin and
                  Brooke Chan and
                  Scott Gray and
                  Nick Ryder and
                  Mikhail Pavlov and
                  Alethea Power and
                  Lukasz Kaiser and
                  Mohammad Bavarian and
                  Clemens Winter and
                  Philippe Tillet and
                  Felipe Petroski Such and
                  Dave Cummings and
                  Matthias Plappert and
                  Fotios Chantzis and
                  Elizabeth Barnes and
                  Ariel Herbert{-}Voss and
                  William Hebgen Guss and
                  Alex Nichol and
                  Alex Paino and
                  Nikolas Tezak and
                  Jie Tang and
                  Igor Babuschkin and
                  Suchir Balaji and
                  Shantanu Jain and
                  William Saunders and
                  Christopher Hesse and
                  Andrew N. Carr and
                  Jan Leike and
                  Joshua Achiam and
                  Vedant Misra and
                  Evan Morikawa and
                  Alec Radford and
                  Matthew Knight and
                  Miles Brundage and
                  Mira Murati and
                  Katie Mayer and
                  Peter Welinder and
                  Bob McGrew and
                  Dario Amodei and
                  Sam McCandlish and
                  Ilya Sutskever and
                  Wojciech Zaremba},
  title        = {Evaluating Large Language Models Trained on Code},
  journal      = {CoRR},
  volume       = {abs/2107.03374},
  year         = {2021}
}

@article{gpqa,
  author       = {David Rein and
                  Betty Li Hou and
                  Asa Cooper Stickland and
                  Jackson Petty and
                  Richard Yuanzhe Pang and
                  Julien Dirani and
                  Julian Michael and
                  Samuel R. Bowman},
  title        = {{GPQA}: A Graduate-Level {Google}-Proof {Q}{\&}{A} Benchmark},
  journal      = {CoRR},
  volume       = {abs/2311.12022},
  year         = {2023}
}

@inproceedings{theoremqa,
  author       = {Wenhu Chen and
                  Ming Yin and
                  Max Ku and
                  Pan Lu and
                  Yixin Wan and
                  Xueguang Ma and
                  Jianyu Xu and
                  Xinyi Wang and
                  Tony Xia},
  title        = {{TheoremQA}: A Theorem-driven Question Answering Dataset},
  booktitle    = {{EMNLP}},
  pages        = {7889--7901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@article{alpacaeval,
  author       = {Yann Dubois and
                  Bal{\'{a}}zs Galambosi and
                  Percy Liang and
                  Tatsunori B. Hashimoto},
  title        = {Length-Controlled AlpacaEval: {A} Simple Way to Debias Automatic Evaluators},
  journal      = {CoRR},
  volume       = {abs/2404.04475},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.04475},
  doi          = {10.48550/ARXIV.2404.04475},
  eprinttype    = {arXiv},
  eprint       = {2404.04475},
  timestamp    = {Wed, 15 May 2024 08:47:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-04475.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ifeval,
  author       = {Jeffrey Zhou and
                  Tianjian Lu and
                  Swaroop Mishra and
                  Siddhartha Brahma and
                  Sujoy Basu and
                  Yi Luan and
                  Denny Zhou and
                  Le Hou},
  title        = {Instruction-Following Evaluation for Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.07911},
  year         = {2023}
}

@article{multiple,
  author       = {Federico Cassano and
                  John Gouwar and
                  Daniel Nguyen and
                  Sydney Nguyen and
                  Luna Phipps{-}Costin and
                  Donald Pinckney and
                  Ming{-}Ho Yee and
                  Yangtian Zi and
                  Carolyn Jane Anderson and
                  Molly Q. Feldman and
                  Arjun Guha and
                  Michael Greenberg and
                  Abhinav Jangda},
  title        = {{MultiPL-E}: {A} Scalable and Polyglot Approach to Benchmarking Neural
                  Code Generation},
  journal      = {{IEEE} Trans. Software Eng.},
  volume       = {49},
  number       = {7},
  pages        = {3675--3691},
  year         = {2023}
}

@article{mmlupro,
  author       = {Yubo Wang and
                  Xueguang Ma and
                  Ge Zhang and
                  Yuansheng Ni and
                  Abhranil Chandra and
                  Shiguang Guo and
                  Weiming Ren and
                  Aaran Arulraj and
                  Xuan He and
                  Ziyan Jiang and
                  Tianle Li and
                  Max Ku and
                  Kai Wang and
                  Alex Zhuang and
                  Rongqi Fan and
                  Xiang Yue and
                  Wenhu Chen},
  title        = {{MMLU-Pro}: {A} More Robust and Challenging Multi-Task Language Understanding
                  Benchmark},
  journal      = {CoRR},
  volume       = {abs/2406.01574},
  year         = {2024}
}

@article{winogrande,
  author       = {Keisuke Sakaguchi and
                  Ronan Le Bras and
                  Chandra Bhagavatula and
                  Yejin Choi},
  title        = {{WinoGrande}: An adversarial winograd schema challenge at scale},
  journal      = {Commun. {ACM}},
  volume       = {64},
  number       = {9},
  pages        = {99--106},
  year         = {2021}
}

@inproceedings{truthfulqa,
  author       = {Stephanie Lin and
                  Jacob Hilton and
                  Owain Evans},
  title        = {{TruthfulQA}: Measuring How Models Mimic Human Falsehoods},
  booktitle    = {{ACL} {(1)}},
  pages        = {3214--3252},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}

@article{jamba,
  author       = {Opher Lieber and
                  Barak Lenz and
                  Hofit Bata and
                  Gal Cohen and
                  Jhonathan Osin and
                  Itay Dalmedigos and
                  Erez Safahi and
                  Shaked Meirom and
                  Yonatan Belinkov and
                  Shai Shalev{-}Shwartz and
                  Omri Abend and
                  Raz Alon and
                  Tomer Asida and
                  Amir Bergman and
                  Roman Glozman and
                  Michael Gokhman and
                  Avashalom Manevich and
                  Nir Ratner and
                  Noam Rozen and
                  Erez Shwartz and
                  Mor Zusman and
                  Yoav Shoham},
  title        = {Jamba: A Hybrid {Transformer-Mamba} Language Model},
  journal      = {CoRR},
  volume       = {abs/2403.19887},
  year         = {2024}
}

@misc{codeqwen,
      title={{Code with CodeQwen1.5}}, 
      author={{Qwen Team}},
      year={2024},
      url={https://qwenlm.github.io/blog/codeqwen1.5/}
}

@misc{phi2,
      title={Phi-2: The surprising power of small language models}, 
      author={Marah Abdin and Jyoti Aneja and Sebastien Bubeck and Caio César Teodoro Mendes and Weizhu Chen and Allie Del Giorno and Ronen Eldan and Sivakanth Gopi and Suriya Gunasekar and Mojan Javaheripi and Piero Kauffmann and Yin Tat Lee and Yuanzhi Li and Anh Nguyen and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Michael Santacroce and Harkirat Singh Behl and Adam Taumann Kalai and Xin Wang and Rachel Ward and Philipp Witte and Cyril Zhang and Yi Zhang},
      year={2024},
      url={https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}
}

@article{minicpm,
  author       = {Shengding Hu and
                  Yuge Tu and
                  Xu Han and
                  Chaoqun He and
                  Ganqu Cui and
                  Xiang Long and
                  Zhi Zheng and
                  Yewei Fang and
                  Yuxiang Huang and
                  Weilin Zhao and
                  Xinrong Zhang and
                  Zhen Leng Thai and
                  Kai Zhang and
                  Chongyi Wang and
                  Yuan Yao and
                  Chenyang Zhao and
                  Jie Zhou and
                  Jie Cai and
                  Zhongwu Zhai and
                  Ning Ding and
                  Chao Jia and
                  Guoyang Zeng and
                  Dahai Li and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {{MiniCPM}: Unveiling the Potential of Small Language Models with Scalable
                  Training Strategies},
  journal      = {CoRR},
  volume       = {abs/2404.06395},
  year         = {2024}
}

@article{phi3,
  author       = {Marah I Abdin and
                  Sam Ade Jacobs and
                  Ammar Ahmad Awan and
                  Jyoti Aneja and
                  Ahmed Awadallah and
                  Hany Awadalla and
                  Nguyen Bach and
                  Amit Bahree and
                  Arash Bakhtiari and
                  Harkirat S. Behl and
                  Alon Benhaim and
                  Misha Bilenko and
                  Johan Bjorck and
                  S{\'{e}}bastien Bubeck and
                  Martin Cai and
                  Caio C{\'{e}}sar Teodoro Mendes and
                  Weizhu Chen and
                  Vishrav Chaudhary and
                  Parul Chopra and
                  Allie Del Giorno and
                  Gustavo de Rosa and
                  Matthew Dixon and
                  Ronen Eldan and
                  Dan Iter and
                  Amit Garg and
                  Abhishek Goswami and
                  Suriya Gunasekar and
                  Emman Haider and
                  Junheng Hao and
                  Russell J. Hewett and
                  Jamie Huynh and
                  Mojan Javaheripi and
                  Xin Jin and
                  Piero Kauffmann and
                  Nikos Karampatziakis and
                  Dongwoo Kim and
                  Mahoud Khademi and
                  Lev Kurilenko and
                  James R. Lee and
                  Yin Tat Lee and
                  Yuanzhi Li and
                  Chen Liang and
                  Weishung Liu and
                  Eric Lin and
                  Zeqi Lin and
                  Piyush Madan and
                  Arindam Mitra and
                  Hardik Modi and
                  Anh Nguyen and
                  Brandon Norick and
                  Barun Patra and
                  Daniel Perez{-}Becker and
                  Thomas Portet and
                  Reid Pryzant and
                  Heyang Qin and
                  Marko Radmilac and
                  Corby Rosset and
                  Sambudha Roy and
                  Olatunji Ruwase and
                  Olli Saarikivi and
                  Amin Saied and
                  Adil Salim and
                  Michael Santacroce and
                  Shital Shah and
                  Ning Shang and
                  Hiteshi Sharma and
                  Xia Song and
                  Masahiro Tanaka and
                  Xin Wang and
                  Rachel Ward and
                  Guanhua Wang and
                  Philipp Witte and
                  Michael Wyatt and
                  Can Xu and
                  Jiahang Xu and
                  Sonali Yadav and
                  Fan Yang and
                  Ziyi Yang and
                  Donghan Yu and
                  Chengruidong Zhang and
                  Cyril Zhang and
                  Jianwen Zhang and
                  Li Lyna Zhang and
                  Yi Zhang and
                  Yue Zhang and
                  Yunan Zhang and
                  Xiren Zhou},
  title        = {Phi-3 Technical Report: {A} Highly Capable Language Model Locally
                  on Your Phone},
  journal      = {CoRR},
  volume       = {abs/2404.14219},
  year         = {2024}
}

@misc{niah,
      title={Needle in a haystack - Pressure testing {LLMs}}, 
      author={Gregory Kamradt},
      year={2023},
      url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack}
}

@article{yuan2024lveval,
  author       = {Tao Yuan and
                  Xuefei Ning and
                  Dong Zhou and
                  Zhijie Yang and
                  Shiyao Li and
                  Minghui Zhuang and
                  Zheyue Tan and
                  Zhuyu Yao and
                  Dahua Lin and
                  Boxun Li and
                  Guohao Dai and
                  Shengen Yan and
                  Yu Wang},
  title        = {{LV-Eval}: {A} Balanced Long-Context Benchmark with 5 Length Levels
                  Up to {256K}},
  journal      = {CoRR},
  volume       = {abs/2402.05136},
  year         = {2024}
}

@article{chatglm4,
    title={{ChatGLM}: A Family of Large Language Models from {GLM-130B} to {GLM-4} All Tools},
    author={Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
  journal      = {CoRR},
  volume       = {abs/2406.12793},
  year         = {2024}
}

@inproceedings{wang2020neural,
  author       = {Changhan Wang and
                  Kyunghyun Cho and
                  Jiatao Gu},
  title        = {Neural Machine Translation with Byte-Level Subwords},
  booktitle    = {{AAAI}},
  pages        = {9154--9160},
  publisher    = {{AAAI} Press},
  year         = {2020}
}

@inproceedings{sennirch2016neural,
  author       = {Rico Sennrich and
                  Barry Haddow and
                  Alexandra Birch},
  title        = {Neural Machine Translation of Rare Words with Subword Units},
  booktitle    = {{ACL} {(1)}},
  publisher    = {The Association for Computer Linguistics},
  year         = {2016}
}

@article{hsieh2024ruler,
  title={{RULER}: What's the Real Context Size of Your Long-Context Language Models?},
  author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
  year={2024},
  journal={CoRR},
  volume={abs/2404.06654},
}



@inproceedings{longalign2024bai,
  author       = {Yushi Bai and
                  Xin Lv and
                  Jiajie Zhang and
                  Yuze He and
                  Ji Qi and
                  Lei Hou and
                  Jie Tang and
                  Yuxiao Dong and
                  Juanzi Li},
  title        = {{LongAlign}: {A} Recipe for Long Context Alignment of Large Language
                  Models},
  booktitle    = {{EMNLP} (Findings)},
  pages        = {1376--1395},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@article{livebench,
  author    = {Colin White and
                  Samuel Dooley and
                  Manley Roberts and
                  Arka Pal and
                  Benjamin Feuer and
                  Siddhartha Jain and
                  Ravid Shwartz{-}Ziv and
                  Neel Jain and
                  Khalid Saifullah and
                  Siddartha Naidu and
                  Chinmay Hegde and
                  Yann LeCun and
                  Tom Goldstein and
                  Willie Neiswanger and
                  Micah Goldblum},
  title     = {{LiveBench}: A Challenging, Contamination-Free {LLM} Benchmark},
  year      = {2024},
  journal   = {CoRR},
  volume    = {abs/2406.19314}
}

@article{blend,
  author       = {Junho Myung and
                  Nayeon Lee and
                  Yi Zhou and
                  Jiho Jin and
                  Rifki Afina Putri and
                  Dimosthenis Antypas and
                  Hsuvas Borkakoty and
                  Eunsu Kim and
                  Carla P{\'{e}}rez{-}Almendros and
                  Abinew Ali Ayele and
                  V{\'{\i}}ctor Guti{\'{e}}rrez{-}Basulto and
                  Yazm{\'{\i}}n Ib{\'{a}}{\~{n}}ez{-}Garc{\'{\i}}a and
                  Hwaran Lee and
                  Shamsuddeen Hassan Muhammad and
                  Ki{-}Woong Park and
                  Anar Sabuhi Rzayev and
                  Nina White and
                  Seid Muhie Yimam and
                  Mohammad Taher Pilehvar and
                  Nedjma Ousidhoum and
                  Jos{\'{e}} Camacho{-}Collados and
                  Alice Oh},
  title        = {BLEnD: {A} Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages},
  year         = {2024},
  journal      = {CoRR},
  volume       = {abs/2406.09948}
}

@article{jiang2024minference,
    title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
    author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
    journal={arXiv preprint arXiv:2407.02490},
    year={2024}
}